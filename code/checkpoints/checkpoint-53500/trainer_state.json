{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9722222222222223,
  "eval_steps": 500,
  "global_step": 53500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002777777777777778,
      "grad_norm": 11.90434741973877,
      "learning_rate": 4.995462962962963e-05,
      "loss": 3.8315,
      "step": 50
    },
    {
      "epoch": 0.005555555555555556,
      "grad_norm": 9.994585037231445,
      "learning_rate": 4.990833333333334e-05,
      "loss": 3.0637,
      "step": 100
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 5.791244983673096,
      "learning_rate": 4.986203703703704e-05,
      "loss": 2.7698,
      "step": 150
    },
    {
      "epoch": 0.011111111111111112,
      "grad_norm": 7.193745136260986,
      "learning_rate": 4.981574074074074e-05,
      "loss": 2.7644,
      "step": 200
    },
    {
      "epoch": 0.013888888888888888,
      "grad_norm": 5.852795600891113,
      "learning_rate": 4.976944444444445e-05,
      "loss": 2.4648,
      "step": 250
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 7.628100872039795,
      "learning_rate": 4.972314814814815e-05,
      "loss": 2.3862,
      "step": 300
    },
    {
      "epoch": 0.019444444444444445,
      "grad_norm": 6.566852569580078,
      "learning_rate": 4.967685185185185e-05,
      "loss": 2.2907,
      "step": 350
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 7.945112228393555,
      "learning_rate": 4.9630555555555557e-05,
      "loss": 2.1561,
      "step": 400
    },
    {
      "epoch": 0.025,
      "grad_norm": 10.629084587097168,
      "learning_rate": 4.958425925925926e-05,
      "loss": 2.1888,
      "step": 450
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 8.369952201843262,
      "learning_rate": 4.953796296296297e-05,
      "loss": 2.0226,
      "step": 500
    },
    {
      "epoch": 0.030555555555555555,
      "grad_norm": 8.114625930786133,
      "learning_rate": 4.949166666666667e-05,
      "loss": 1.8659,
      "step": 550
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 6.23626184463501,
      "learning_rate": 4.944537037037038e-05,
      "loss": 1.6814,
      "step": 600
    },
    {
      "epoch": 0.03611111111111111,
      "grad_norm": 10.867777824401855,
      "learning_rate": 4.9399074074074076e-05,
      "loss": 1.6589,
      "step": 650
    },
    {
      "epoch": 0.03888888888888889,
      "grad_norm": 5.624894618988037,
      "learning_rate": 4.935277777777778e-05,
      "loss": 1.5406,
      "step": 700
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 6.072667121887207,
      "learning_rate": 4.930648148148148e-05,
      "loss": 1.4754,
      "step": 750
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 5.344645977020264,
      "learning_rate": 4.9260185185185186e-05,
      "loss": 1.3076,
      "step": 800
    },
    {
      "epoch": 0.04722222222222222,
      "grad_norm": 4.819239616394043,
      "learning_rate": 4.921388888888889e-05,
      "loss": 1.2112,
      "step": 850
    },
    {
      "epoch": 0.05,
      "grad_norm": 3.6621224880218506,
      "learning_rate": 4.916759259259259e-05,
      "loss": 1.0715,
      "step": 900
    },
    {
      "epoch": 0.05277777777777778,
      "grad_norm": 4.926736831665039,
      "learning_rate": 4.91212962962963e-05,
      "loss": 1.0234,
      "step": 950
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 5.937860488891602,
      "learning_rate": 4.907500000000001e-05,
      "loss": 1.067,
      "step": 1000
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 3.4495182037353516,
      "learning_rate": 4.9028703703703706e-05,
      "loss": 1.036,
      "step": 1050
    },
    {
      "epoch": 0.06111111111111111,
      "grad_norm": 3.1660964488983154,
      "learning_rate": 4.898240740740741e-05,
      "loss": 0.9747,
      "step": 1100
    },
    {
      "epoch": 0.06388888888888888,
      "grad_norm": 2.293264150619507,
      "learning_rate": 4.8936111111111116e-05,
      "loss": 1.0405,
      "step": 1150
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 2.2124826908111572,
      "learning_rate": 4.8889814814814815e-05,
      "loss": 0.96,
      "step": 1200
    },
    {
      "epoch": 0.06944444444444445,
      "grad_norm": 10.01025104522705,
      "learning_rate": 4.884351851851852e-05,
      "loss": 0.9162,
      "step": 1250
    },
    {
      "epoch": 0.07222222222222222,
      "grad_norm": 3.19781494140625,
      "learning_rate": 4.879722222222222e-05,
      "loss": 0.9349,
      "step": 1300
    },
    {
      "epoch": 0.075,
      "grad_norm": 7.701310634613037,
      "learning_rate": 4.875092592592593e-05,
      "loss": 0.8953,
      "step": 1350
    },
    {
      "epoch": 0.07777777777777778,
      "grad_norm": 0.8940068483352661,
      "learning_rate": 4.8704629629629636e-05,
      "loss": 0.8498,
      "step": 1400
    },
    {
      "epoch": 0.08055555555555556,
      "grad_norm": 2.335496425628662,
      "learning_rate": 4.8658333333333335e-05,
      "loss": 0.8817,
      "step": 1450
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 3.0190927982330322,
      "learning_rate": 4.861203703703704e-05,
      "loss": 0.8544,
      "step": 1500
    },
    {
      "epoch": 0.08611111111111111,
      "grad_norm": 2.9995172023773193,
      "learning_rate": 4.8565740740740745e-05,
      "loss": 0.878,
      "step": 1550
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 5.0787177085876465,
      "learning_rate": 4.8519444444444444e-05,
      "loss": 0.855,
      "step": 1600
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 2.561572313308716,
      "learning_rate": 4.847314814814815e-05,
      "loss": 0.8712,
      "step": 1650
    },
    {
      "epoch": 0.09444444444444444,
      "grad_norm": 2.381718158721924,
      "learning_rate": 4.8426851851851855e-05,
      "loss": 0.846,
      "step": 1700
    },
    {
      "epoch": 0.09722222222222222,
      "grad_norm": 1.3040869235992432,
      "learning_rate": 4.838055555555556e-05,
      "loss": 0.873,
      "step": 1750
    },
    {
      "epoch": 0.1,
      "grad_norm": 2.110292434692383,
      "learning_rate": 4.8334259259259265e-05,
      "loss": 0.7878,
      "step": 1800
    },
    {
      "epoch": 0.10277777777777777,
      "grad_norm": 2.234992027282715,
      "learning_rate": 4.8287962962962964e-05,
      "loss": 0.8274,
      "step": 1850
    },
    {
      "epoch": 0.10555555555555556,
      "grad_norm": 1.5169546604156494,
      "learning_rate": 4.824166666666667e-05,
      "loss": 0.8113,
      "step": 1900
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 4.264376163482666,
      "learning_rate": 4.8195370370370375e-05,
      "loss": 0.734,
      "step": 1950
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 3.6047515869140625,
      "learning_rate": 4.814907407407407e-05,
      "loss": 0.7788,
      "step": 2000
    },
    {
      "epoch": 0.11388888888888889,
      "grad_norm": 1.8734791278839111,
      "learning_rate": 4.810277777777778e-05,
      "loss": 0.7401,
      "step": 2050
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 2.258662700653076,
      "learning_rate": 4.8056481481481484e-05,
      "loss": 0.7536,
      "step": 2100
    },
    {
      "epoch": 0.11944444444444445,
      "grad_norm": 3.6190125942230225,
      "learning_rate": 4.801018518518519e-05,
      "loss": 0.7773,
      "step": 2150
    },
    {
      "epoch": 0.12222222222222222,
      "grad_norm": 2.3901498317718506,
      "learning_rate": 4.7963888888888894e-05,
      "loss": 0.8087,
      "step": 2200
    },
    {
      "epoch": 0.125,
      "grad_norm": 2.125985860824585,
      "learning_rate": 4.79175925925926e-05,
      "loss": 0.7424,
      "step": 2250
    },
    {
      "epoch": 0.12777777777777777,
      "grad_norm": 4.1736602783203125,
      "learning_rate": 4.78712962962963e-05,
      "loss": 0.7583,
      "step": 2300
    },
    {
      "epoch": 0.13055555555555556,
      "grad_norm": 2.5127482414245605,
      "learning_rate": 4.7825000000000004e-05,
      "loss": 0.7139,
      "step": 2350
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 1.8804508447647095,
      "learning_rate": 4.77787037037037e-05,
      "loss": 0.5157,
      "step": 2400
    },
    {
      "epoch": 0.1361111111111111,
      "grad_norm": 1.4621845483779907,
      "learning_rate": 4.773240740740741e-05,
      "loss": 0.5125,
      "step": 2450
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 3.3339691162109375,
      "learning_rate": 4.768611111111111e-05,
      "loss": 0.4634,
      "step": 2500
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 1.6067200899124146,
      "learning_rate": 4.763981481481481e-05,
      "loss": 0.3835,
      "step": 2550
    },
    {
      "epoch": 0.14444444444444443,
      "grad_norm": 2.2374703884124756,
      "learning_rate": 4.7593518518518524e-05,
      "loss": 0.4753,
      "step": 2600
    },
    {
      "epoch": 0.14722222222222223,
      "grad_norm": 2.781956195831299,
      "learning_rate": 4.754722222222223e-05,
      "loss": 0.3973,
      "step": 2650
    },
    {
      "epoch": 0.15,
      "grad_norm": 2.3037872314453125,
      "learning_rate": 4.750092592592593e-05,
      "loss": 0.4227,
      "step": 2700
    },
    {
      "epoch": 0.1527777777777778,
      "grad_norm": 4.147977828979492,
      "learning_rate": 4.745462962962963e-05,
      "loss": 0.397,
      "step": 2750
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 1.2980574369430542,
      "learning_rate": 4.740833333333334e-05,
      "loss": 0.4004,
      "step": 2800
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 2.879303455352783,
      "learning_rate": 4.736203703703704e-05,
      "loss": 0.4105,
      "step": 2850
    },
    {
      "epoch": 0.16111111111111112,
      "grad_norm": 1.596475601196289,
      "learning_rate": 4.731574074074074e-05,
      "loss": 0.3758,
      "step": 2900
    },
    {
      "epoch": 0.1638888888888889,
      "grad_norm": 2.392899990081787,
      "learning_rate": 4.726944444444444e-05,
      "loss": 0.373,
      "step": 2950
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.296296238899231,
      "learning_rate": 4.722314814814815e-05,
      "loss": 0.3602,
      "step": 3000
    },
    {
      "epoch": 0.16944444444444445,
      "grad_norm": 2.2044966220855713,
      "learning_rate": 4.717685185185186e-05,
      "loss": 0.3641,
      "step": 3050
    },
    {
      "epoch": 0.17222222222222222,
      "grad_norm": 1.9878815412521362,
      "learning_rate": 4.713055555555556e-05,
      "loss": 0.3957,
      "step": 3100
    },
    {
      "epoch": 0.175,
      "grad_norm": 3.6222329139709473,
      "learning_rate": 4.708425925925926e-05,
      "loss": 0.3624,
      "step": 3150
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 2.109740972518921,
      "learning_rate": 4.703796296296297e-05,
      "loss": 0.334,
      "step": 3200
    },
    {
      "epoch": 0.18055555555555555,
      "grad_norm": 1.6526023149490356,
      "learning_rate": 4.6991666666666666e-05,
      "loss": 0.357,
      "step": 3250
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 2.0113046169281006,
      "learning_rate": 4.694537037037037e-05,
      "loss": 0.3434,
      "step": 3300
    },
    {
      "epoch": 0.18611111111111112,
      "grad_norm": 2.067837715148926,
      "learning_rate": 4.689907407407408e-05,
      "loss": 0.3571,
      "step": 3350
    },
    {
      "epoch": 0.18888888888888888,
      "grad_norm": 2.1185035705566406,
      "learning_rate": 4.685277777777778e-05,
      "loss": 0.3683,
      "step": 3400
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 3.493863105773926,
      "learning_rate": 4.680648148148149e-05,
      "loss": 0.4173,
      "step": 3450
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 2.392904758453369,
      "learning_rate": 4.6760185185185186e-05,
      "loss": 0.3655,
      "step": 3500
    },
    {
      "epoch": 0.19722222222222222,
      "grad_norm": 1.255173683166504,
      "learning_rate": 4.671388888888889e-05,
      "loss": 0.3086,
      "step": 3550
    },
    {
      "epoch": 0.2,
      "grad_norm": 2.2831361293792725,
      "learning_rate": 4.6667592592592597e-05,
      "loss": 0.3654,
      "step": 3600
    },
    {
      "epoch": 0.20277777777777778,
      "grad_norm": 1.2914279699325562,
      "learning_rate": 4.6621296296296295e-05,
      "loss": 0.3222,
      "step": 3650
    },
    {
      "epoch": 0.20555555555555555,
      "grad_norm": 1.9168161153793335,
      "learning_rate": 4.6575e-05,
      "loss": 0.3691,
      "step": 3700
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 1.934829831123352,
      "learning_rate": 4.6528703703703706e-05,
      "loss": 0.355,
      "step": 3750
    },
    {
      "epoch": 0.2111111111111111,
      "grad_norm": 2.5414042472839355,
      "learning_rate": 4.648240740740741e-05,
      "loss": 0.3636,
      "step": 3800
    },
    {
      "epoch": 0.21388888888888888,
      "grad_norm": 1.9105583429336548,
      "learning_rate": 4.6436111111111116e-05,
      "loss": 0.2578,
      "step": 3850
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 3.155217170715332,
      "learning_rate": 4.638981481481482e-05,
      "loss": 0.3172,
      "step": 3900
    },
    {
      "epoch": 0.21944444444444444,
      "grad_norm": 1.8437557220458984,
      "learning_rate": 4.634351851851852e-05,
      "loss": 0.3587,
      "step": 3950
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 1.5729891061782837,
      "learning_rate": 4.6297222222222226e-05,
      "loss": 0.3386,
      "step": 4000
    },
    {
      "epoch": 0.225,
      "grad_norm": 1.5842921733856201,
      "learning_rate": 4.625092592592593e-05,
      "loss": 0.3213,
      "step": 4050
    },
    {
      "epoch": 0.22777777777777777,
      "grad_norm": 2.4429404735565186,
      "learning_rate": 4.620462962962963e-05,
      "loss": 0.3434,
      "step": 4100
    },
    {
      "epoch": 0.23055555555555557,
      "grad_norm": 2.6322457790374756,
      "learning_rate": 4.6158333333333335e-05,
      "loss": 0.3511,
      "step": 4150
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 2.61795711517334,
      "learning_rate": 4.611203703703704e-05,
      "loss": 0.3388,
      "step": 4200
    },
    {
      "epoch": 0.2361111111111111,
      "grad_norm": 3.5542776584625244,
      "learning_rate": 4.6065740740740746e-05,
      "loss": 0.3443,
      "step": 4250
    },
    {
      "epoch": 0.2388888888888889,
      "grad_norm": 1.6288270950317383,
      "learning_rate": 4.601944444444445e-05,
      "loss": 0.3396,
      "step": 4300
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 1.8815956115722656,
      "learning_rate": 4.597314814814815e-05,
      "loss": 0.3577,
      "step": 4350
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 1.8966288566589355,
      "learning_rate": 4.5926851851851855e-05,
      "loss": 0.3874,
      "step": 4400
    },
    {
      "epoch": 0.24722222222222223,
      "grad_norm": 1.6303822994232178,
      "learning_rate": 4.588055555555556e-05,
      "loss": 0.3139,
      "step": 4450
    },
    {
      "epoch": 0.25,
      "grad_norm": 1.5222978591918945,
      "learning_rate": 4.583425925925926e-05,
      "loss": 0.3297,
      "step": 4500
    },
    {
      "epoch": 0.25277777777777777,
      "grad_norm": 1.4459830522537231,
      "learning_rate": 4.5787962962962964e-05,
      "loss": 0.3564,
      "step": 4550
    },
    {
      "epoch": 0.25555555555555554,
      "grad_norm": 1.8329885005950928,
      "learning_rate": 4.574166666666667e-05,
      "loss": 0.3499,
      "step": 4600
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 1.015103816986084,
      "learning_rate": 4.5695370370370375e-05,
      "loss": 0.3366,
      "step": 4650
    },
    {
      "epoch": 0.2611111111111111,
      "grad_norm": 4.081194877624512,
      "learning_rate": 4.564907407407408e-05,
      "loss": 0.3464,
      "step": 4700
    },
    {
      "epoch": 0.2638888888888889,
      "grad_norm": 1.619686245918274,
      "learning_rate": 4.560277777777778e-05,
      "loss": 0.3358,
      "step": 4750
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.6128184795379639,
      "learning_rate": 4.5556481481481484e-05,
      "loss": 0.2965,
      "step": 4800
    },
    {
      "epoch": 0.26944444444444443,
      "grad_norm": 1.7692668437957764,
      "learning_rate": 4.551018518518519e-05,
      "loss": 0.3533,
      "step": 4850
    },
    {
      "epoch": 0.2722222222222222,
      "grad_norm": 2.3894550800323486,
      "learning_rate": 4.546388888888889e-05,
      "loss": 0.3117,
      "step": 4900
    },
    {
      "epoch": 0.275,
      "grad_norm": 2.816666841506958,
      "learning_rate": 4.541759259259259e-05,
      "loss": 0.3447,
      "step": 4950
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 1.1870627403259277,
      "learning_rate": 4.53712962962963e-05,
      "loss": 0.3086,
      "step": 5000
    },
    {
      "epoch": 0.28055555555555556,
      "grad_norm": 1.7868027687072754,
      "learning_rate": 4.5325000000000004e-05,
      "loss": 0.3164,
      "step": 5050
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 1.1931331157684326,
      "learning_rate": 4.527870370370371e-05,
      "loss": 0.3139,
      "step": 5100
    },
    {
      "epoch": 0.2861111111111111,
      "grad_norm": 1.9501261711120605,
      "learning_rate": 4.5232407407407415e-05,
      "loss": 0.3383,
      "step": 5150
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 2.462043285369873,
      "learning_rate": 4.518611111111111e-05,
      "loss": 0.3276,
      "step": 5200
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 1.9319570064544678,
      "learning_rate": 4.513981481481482e-05,
      "loss": 0.3366,
      "step": 5250
    },
    {
      "epoch": 0.29444444444444445,
      "grad_norm": 1.2537931203842163,
      "learning_rate": 4.509351851851852e-05,
      "loss": 0.2953,
      "step": 5300
    },
    {
      "epoch": 0.2972222222222222,
      "grad_norm": 2.1216418743133545,
      "learning_rate": 4.504722222222222e-05,
      "loss": 0.3129,
      "step": 5350
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.6016640663146973,
      "learning_rate": 4.500092592592593e-05,
      "loss": 0.2961,
      "step": 5400
    },
    {
      "epoch": 0.30277777777777776,
      "grad_norm": 1.4206061363220215,
      "learning_rate": 4.495462962962963e-05,
      "loss": 0.3142,
      "step": 5450
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 0.7738444209098816,
      "learning_rate": 4.490833333333334e-05,
      "loss": 0.3187,
      "step": 5500
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 1.9208118915557861,
      "learning_rate": 4.4862037037037044e-05,
      "loss": 0.3043,
      "step": 5550
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 1.2662296295166016,
      "learning_rate": 4.481574074074074e-05,
      "loss": 0.2968,
      "step": 5600
    },
    {
      "epoch": 0.3138888888888889,
      "grad_norm": 2.1082067489624023,
      "learning_rate": 4.476944444444445e-05,
      "loss": 0.2902,
      "step": 5650
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 1.744678020477295,
      "learning_rate": 4.472314814814815e-05,
      "loss": 0.3125,
      "step": 5700
    },
    {
      "epoch": 0.3194444444444444,
      "grad_norm": 1.518040418624878,
      "learning_rate": 4.467685185185185e-05,
      "loss": 0.2221,
      "step": 5750
    },
    {
      "epoch": 0.32222222222222224,
      "grad_norm": 2.758617639541626,
      "learning_rate": 4.463055555555556e-05,
      "loss": 0.3018,
      "step": 5800
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.9302141070365906,
      "learning_rate": 4.458425925925926e-05,
      "loss": 0.2894,
      "step": 5850
    },
    {
      "epoch": 0.3277777777777778,
      "grad_norm": 2.7608673572540283,
      "learning_rate": 4.453796296296297e-05,
      "loss": 0.3445,
      "step": 5900
    },
    {
      "epoch": 0.33055555555555555,
      "grad_norm": 4.220006942749023,
      "learning_rate": 4.449166666666667e-05,
      "loss": 0.3064,
      "step": 5950
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 2.053678512573242,
      "learning_rate": 4.444537037037037e-05,
      "loss": 0.2935,
      "step": 6000
    },
    {
      "epoch": 0.33611111111111114,
      "grad_norm": 1.8834543228149414,
      "learning_rate": 4.439907407407408e-05,
      "loss": 0.3273,
      "step": 6050
    },
    {
      "epoch": 0.3388888888888889,
      "grad_norm": 2.5834851264953613,
      "learning_rate": 4.435277777777778e-05,
      "loss": 0.3128,
      "step": 6100
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 2.062054395675659,
      "learning_rate": 4.430648148148148e-05,
      "loss": 0.3075,
      "step": 6150
    },
    {
      "epoch": 0.34444444444444444,
      "grad_norm": 1.591145634651184,
      "learning_rate": 4.4260185185185186e-05,
      "loss": 0.3119,
      "step": 6200
    },
    {
      "epoch": 0.3472222222222222,
      "grad_norm": 0.7484476566314697,
      "learning_rate": 4.421388888888889e-05,
      "loss": 0.3122,
      "step": 6250
    },
    {
      "epoch": 0.35,
      "grad_norm": 3.0970458984375,
      "learning_rate": 4.41675925925926e-05,
      "loss": 0.3208,
      "step": 6300
    },
    {
      "epoch": 0.3527777777777778,
      "grad_norm": 3.2939603328704834,
      "learning_rate": 4.41212962962963e-05,
      "loss": 0.3161,
      "step": 6350
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.8258692622184753,
      "learning_rate": 4.4075e-05,
      "loss": 0.2751,
      "step": 6400
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 1.6044528484344482,
      "learning_rate": 4.4028703703703706e-05,
      "loss": 0.3074,
      "step": 6450
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 1.290650725364685,
      "learning_rate": 4.398240740740741e-05,
      "loss": 0.2895,
      "step": 6500
    },
    {
      "epoch": 0.3638888888888889,
      "grad_norm": 3.2627933025360107,
      "learning_rate": 4.393611111111111e-05,
      "loss": 0.3246,
      "step": 6550
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 1.8275476694107056,
      "learning_rate": 4.3889814814814815e-05,
      "loss": 0.2826,
      "step": 6600
    },
    {
      "epoch": 0.36944444444444446,
      "grad_norm": 2.8104305267333984,
      "learning_rate": 4.384351851851852e-05,
      "loss": 0.3286,
      "step": 6650
    },
    {
      "epoch": 0.37222222222222223,
      "grad_norm": 3.2321691513061523,
      "learning_rate": 4.3797222222222226e-05,
      "loss": 0.3013,
      "step": 6700
    },
    {
      "epoch": 0.375,
      "grad_norm": 1.4183427095413208,
      "learning_rate": 4.375092592592593e-05,
      "loss": 0.2911,
      "step": 6750
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 0.8425576090812683,
      "learning_rate": 4.370462962962964e-05,
      "loss": 0.2949,
      "step": 6800
    },
    {
      "epoch": 0.38055555555555554,
      "grad_norm": 1.3846036195755005,
      "learning_rate": 4.3658333333333335e-05,
      "loss": 0.3576,
      "step": 6850
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 1.1878435611724854,
      "learning_rate": 4.361203703703704e-05,
      "loss": 0.3178,
      "step": 6900
    },
    {
      "epoch": 0.3861111111111111,
      "grad_norm": 2.318103551864624,
      "learning_rate": 4.356574074074074e-05,
      "loss": 0.2909,
      "step": 6950
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 1.3119354248046875,
      "learning_rate": 4.3519444444444444e-05,
      "loss": 0.2764,
      "step": 7000
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 3.448923349380493,
      "learning_rate": 4.347314814814815e-05,
      "loss": 0.3506,
      "step": 7050
    },
    {
      "epoch": 0.39444444444444443,
      "grad_norm": 0.9579792022705078,
      "learning_rate": 4.3426851851851855e-05,
      "loss": 0.2677,
      "step": 7100
    },
    {
      "epoch": 0.3972222222222222,
      "grad_norm": 2.0937817096710205,
      "learning_rate": 4.338055555555556e-05,
      "loss": 0.3239,
      "step": 7150
    },
    {
      "epoch": 0.4,
      "grad_norm": 3.0648999214172363,
      "learning_rate": 4.3334259259259266e-05,
      "loss": 0.2974,
      "step": 7200
    },
    {
      "epoch": 0.4027777777777778,
      "grad_norm": 2.7907726764678955,
      "learning_rate": 4.3287962962962964e-05,
      "loss": 0.2908,
      "step": 7250
    },
    {
      "epoch": 0.40555555555555556,
      "grad_norm": 7.466148853302002,
      "learning_rate": 4.324166666666667e-05,
      "loss": 0.2958,
      "step": 7300
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 1.776694893836975,
      "learning_rate": 4.3195370370370375e-05,
      "loss": 0.297,
      "step": 7350
    },
    {
      "epoch": 0.4111111111111111,
      "grad_norm": 1.601616621017456,
      "learning_rate": 4.3149074074074074e-05,
      "loss": 0.3074,
      "step": 7400
    },
    {
      "epoch": 0.41388888888888886,
      "grad_norm": 2.141195297241211,
      "learning_rate": 4.310277777777778e-05,
      "loss": 0.2729,
      "step": 7450
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 1.4580155611038208,
      "learning_rate": 4.3056481481481484e-05,
      "loss": 0.3218,
      "step": 7500
    },
    {
      "epoch": 0.41944444444444445,
      "grad_norm": 2.3513503074645996,
      "learning_rate": 4.301018518518519e-05,
      "loss": 0.2885,
      "step": 7550
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 4.191890716552734,
      "learning_rate": 4.2963888888888895e-05,
      "loss": 0.3176,
      "step": 7600
    },
    {
      "epoch": 0.425,
      "grad_norm": 1.728369951248169,
      "learning_rate": 4.2917592592592593e-05,
      "loss": 0.3155,
      "step": 7650
    },
    {
      "epoch": 0.42777777777777776,
      "grad_norm": 2.6605756282806396,
      "learning_rate": 4.28712962962963e-05,
      "loss": 0.3239,
      "step": 7700
    },
    {
      "epoch": 0.4305555555555556,
      "grad_norm": 2.0919907093048096,
      "learning_rate": 4.2825000000000004e-05,
      "loss": 0.2779,
      "step": 7750
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 2.1255626678466797,
      "learning_rate": 4.27787037037037e-05,
      "loss": 0.2326,
      "step": 7800
    },
    {
      "epoch": 0.4361111111111111,
      "grad_norm": 1.5477603673934937,
      "learning_rate": 4.273240740740741e-05,
      "loss": 0.3144,
      "step": 7850
    },
    {
      "epoch": 0.4388888888888889,
      "grad_norm": 2.840989589691162,
      "learning_rate": 4.2686111111111113e-05,
      "loss": 0.298,
      "step": 7900
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 1.6978791952133179,
      "learning_rate": 4.263981481481482e-05,
      "loss": 0.2728,
      "step": 7950
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.7767333984375,
      "learning_rate": 4.2593518518518524e-05,
      "loss": 0.3114,
      "step": 8000
    },
    {
      "epoch": 0.44722222222222224,
      "grad_norm": 1.0417368412017822,
      "learning_rate": 4.254722222222222e-05,
      "loss": 0.2952,
      "step": 8050
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.715996503829956,
      "learning_rate": 4.250092592592593e-05,
      "loss": 0.2524,
      "step": 8100
    },
    {
      "epoch": 0.4527777777777778,
      "grad_norm": 1.7930693626403809,
      "learning_rate": 4.245462962962963e-05,
      "loss": 0.2513,
      "step": 8150
    },
    {
      "epoch": 0.45555555555555555,
      "grad_norm": 1.6801648139953613,
      "learning_rate": 4.240833333333333e-05,
      "loss": 0.2981,
      "step": 8200
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 3.4061062335968018,
      "learning_rate": 4.236203703703704e-05,
      "loss": 0.3178,
      "step": 8250
    },
    {
      "epoch": 0.46111111111111114,
      "grad_norm": 1.2933375835418701,
      "learning_rate": 4.231574074074074e-05,
      "loss": 0.2729,
      "step": 8300
    },
    {
      "epoch": 0.4638888888888889,
      "grad_norm": 0.9244004487991333,
      "learning_rate": 4.226944444444445e-05,
      "loss": 0.2658,
      "step": 8350
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 2.8427724838256836,
      "learning_rate": 4.222314814814815e-05,
      "loss": 0.3158,
      "step": 8400
    },
    {
      "epoch": 0.46944444444444444,
      "grad_norm": 1.3804445266723633,
      "learning_rate": 4.217685185185186e-05,
      "loss": 0.278,
      "step": 8450
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 0.8990980386734009,
      "learning_rate": 4.213055555555556e-05,
      "loss": 0.303,
      "step": 8500
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.865043580532074,
      "learning_rate": 4.208425925925926e-05,
      "loss": 0.2549,
      "step": 8550
    },
    {
      "epoch": 0.4777777777777778,
      "grad_norm": 1.0850179195404053,
      "learning_rate": 4.203796296296296e-05,
      "loss": 0.2404,
      "step": 8600
    },
    {
      "epoch": 0.48055555555555557,
      "grad_norm": 1.9908527135849,
      "learning_rate": 4.1991666666666666e-05,
      "loss": 0.2861,
      "step": 8650
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 1.052480697631836,
      "learning_rate": 4.194537037037037e-05,
      "loss": 0.2649,
      "step": 8700
    },
    {
      "epoch": 0.4861111111111111,
      "grad_norm": 1.007502555847168,
      "learning_rate": 4.189907407407408e-05,
      "loss": 0.3023,
      "step": 8750
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 1.6367888450622559,
      "learning_rate": 4.185277777777778e-05,
      "loss": 0.2806,
      "step": 8800
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 1.3465275764465332,
      "learning_rate": 4.180648148148149e-05,
      "loss": 0.3037,
      "step": 8850
    },
    {
      "epoch": 0.49444444444444446,
      "grad_norm": 1.3020774126052856,
      "learning_rate": 4.1760185185185186e-05,
      "loss": 0.269,
      "step": 8900
    },
    {
      "epoch": 0.49722222222222223,
      "grad_norm": 2.019205331802368,
      "learning_rate": 4.171388888888889e-05,
      "loss": 0.2561,
      "step": 8950
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.2342497110366821,
      "learning_rate": 4.16675925925926e-05,
      "loss": 0.2653,
      "step": 9000
    },
    {
      "epoch": 0.5027777777777778,
      "grad_norm": 0.8639692664146423,
      "learning_rate": 4.1621296296296296e-05,
      "loss": 0.302,
      "step": 9050
    },
    {
      "epoch": 0.5055555555555555,
      "grad_norm": 1.1217161417007446,
      "learning_rate": 4.1575e-05,
      "loss": 0.284,
      "step": 9100
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 2.2488553524017334,
      "learning_rate": 4.1528703703703706e-05,
      "loss": 0.3021,
      "step": 9150
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 1.4010568857192993,
      "learning_rate": 4.148240740740741e-05,
      "loss": 0.2909,
      "step": 9200
    },
    {
      "epoch": 0.5138888888888888,
      "grad_norm": 0.8980134129524231,
      "learning_rate": 4.143611111111112e-05,
      "loss": 0.2511,
      "step": 9250
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 1.432392954826355,
      "learning_rate": 4.1389814814814815e-05,
      "loss": 0.2564,
      "step": 9300
    },
    {
      "epoch": 0.5194444444444445,
      "grad_norm": 2.1808364391326904,
      "learning_rate": 4.134351851851852e-05,
      "loss": 0.2654,
      "step": 9350
    },
    {
      "epoch": 0.5222222222222223,
      "grad_norm": 3.270538568496704,
      "learning_rate": 4.1297222222222226e-05,
      "loss": 0.2534,
      "step": 9400
    },
    {
      "epoch": 0.525,
      "grad_norm": 3.11403489112854,
      "learning_rate": 4.1250925925925925e-05,
      "loss": 0.2578,
      "step": 9450
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 1.26664137840271,
      "learning_rate": 4.120462962962963e-05,
      "loss": 0.2899,
      "step": 9500
    },
    {
      "epoch": 0.5305555555555556,
      "grad_norm": 1.8212305307388306,
      "learning_rate": 4.1158333333333335e-05,
      "loss": 0.2974,
      "step": 9550
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 3.789177656173706,
      "learning_rate": 4.111203703703704e-05,
      "loss": 0.2554,
      "step": 9600
    },
    {
      "epoch": 0.5361111111111111,
      "grad_norm": 3.8148913383483887,
      "learning_rate": 4.1065740740740746e-05,
      "loss": 0.2631,
      "step": 9650
    },
    {
      "epoch": 0.5388888888888889,
      "grad_norm": 2.7961628437042236,
      "learning_rate": 4.1019444444444445e-05,
      "loss": 0.3064,
      "step": 9700
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 1.4347652196884155,
      "learning_rate": 4.097314814814815e-05,
      "loss": 0.274,
      "step": 9750
    },
    {
      "epoch": 0.5444444444444444,
      "grad_norm": 0.9027931690216064,
      "learning_rate": 4.0926851851851855e-05,
      "loss": 0.3353,
      "step": 9800
    },
    {
      "epoch": 0.5472222222222223,
      "grad_norm": 2.0222625732421875,
      "learning_rate": 4.0880555555555554e-05,
      "loss": 0.246,
      "step": 9850
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.2183588743209839,
      "learning_rate": 4.083425925925926e-05,
      "loss": 0.2885,
      "step": 9900
    },
    {
      "epoch": 0.5527777777777778,
      "grad_norm": 1.8204513788223267,
      "learning_rate": 4.0787962962962965e-05,
      "loss": 0.2565,
      "step": 9950
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.33502256870269775,
      "learning_rate": 4.074166666666667e-05,
      "loss": 0.2879,
      "step": 10000
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 8.440754890441895,
      "learning_rate": 4.0695370370370375e-05,
      "loss": 0.2563,
      "step": 10050
    },
    {
      "epoch": 0.5611111111111111,
      "grad_norm": 1.4925537109375,
      "learning_rate": 4.064907407407408e-05,
      "loss": 0.2706,
      "step": 10100
    },
    {
      "epoch": 0.5638888888888889,
      "grad_norm": 3.6785964965820312,
      "learning_rate": 4.060277777777778e-05,
      "loss": 0.2661,
      "step": 10150
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 1.21270751953125,
      "learning_rate": 4.0556481481481484e-05,
      "loss": 0.2482,
      "step": 10200
    },
    {
      "epoch": 0.5694444444444444,
      "grad_norm": 1.2252721786499023,
      "learning_rate": 4.051018518518518e-05,
      "loss": 0.2124,
      "step": 10250
    },
    {
      "epoch": 0.5722222222222222,
      "grad_norm": 2.0005409717559814,
      "learning_rate": 4.046388888888889e-05,
      "loss": 0.2048,
      "step": 10300
    },
    {
      "epoch": 0.575,
      "grad_norm": 3.149928331375122,
      "learning_rate": 4.0417592592592594e-05,
      "loss": 0.2861,
      "step": 10350
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 1.276764154434204,
      "learning_rate": 4.03712962962963e-05,
      "loss": 0.2712,
      "step": 10400
    },
    {
      "epoch": 0.5805555555555556,
      "grad_norm": 1.4577713012695312,
      "learning_rate": 4.0325000000000004e-05,
      "loss": 0.2864,
      "step": 10450
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 1.3508495092391968,
      "learning_rate": 4.027870370370371e-05,
      "loss": 0.267,
      "step": 10500
    },
    {
      "epoch": 0.5861111111111111,
      "grad_norm": 2.6986379623413086,
      "learning_rate": 4.023240740740741e-05,
      "loss": 0.2585,
      "step": 10550
    },
    {
      "epoch": 0.5888888888888889,
      "grad_norm": 1.304168462753296,
      "learning_rate": 4.0186111111111114e-05,
      "loss": 0.3019,
      "step": 10600
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.7338355779647827,
      "learning_rate": 4.013981481481482e-05,
      "loss": 0.2583,
      "step": 10650
    },
    {
      "epoch": 0.5944444444444444,
      "grad_norm": 1.0981814861297607,
      "learning_rate": 4.009351851851852e-05,
      "loss": 0.3062,
      "step": 10700
    },
    {
      "epoch": 0.5972222222222222,
      "grad_norm": 2.462315559387207,
      "learning_rate": 4.004722222222222e-05,
      "loss": 0.2426,
      "step": 10750
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.022799253463745,
      "learning_rate": 4.000092592592593e-05,
      "loss": 0.2702,
      "step": 10800
    },
    {
      "epoch": 0.6027777777777777,
      "grad_norm": 1.084491491317749,
      "learning_rate": 3.9954629629629634e-05,
      "loss": 0.2882,
      "step": 10850
    },
    {
      "epoch": 0.6055555555555555,
      "grad_norm": 1.298224925994873,
      "learning_rate": 3.990833333333334e-05,
      "loss": 0.2571,
      "step": 10900
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 1.9334675073623657,
      "learning_rate": 3.986203703703704e-05,
      "loss": 0.237,
      "step": 10950
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 2.364267110824585,
      "learning_rate": 3.981574074074074e-05,
      "loss": 0.257,
      "step": 11000
    },
    {
      "epoch": 0.6138888888888889,
      "grad_norm": 1.6096408367156982,
      "learning_rate": 3.976944444444445e-05,
      "loss": 0.2742,
      "step": 11050
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 1.1969597339630127,
      "learning_rate": 3.972314814814815e-05,
      "loss": 0.246,
      "step": 11100
    },
    {
      "epoch": 0.6194444444444445,
      "grad_norm": 4.25553035736084,
      "learning_rate": 3.967685185185185e-05,
      "loss": 0.2971,
      "step": 11150
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 1.543227195739746,
      "learning_rate": 3.963055555555556e-05,
      "loss": 0.2626,
      "step": 11200
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.8771966099739075,
      "learning_rate": 3.958425925925926e-05,
      "loss": 0.3186,
      "step": 11250
    },
    {
      "epoch": 0.6277777777777778,
      "grad_norm": 2.223451614379883,
      "learning_rate": 3.953796296296297e-05,
      "loss": 0.2723,
      "step": 11300
    },
    {
      "epoch": 0.6305555555555555,
      "grad_norm": 1.2651323080062866,
      "learning_rate": 3.9491666666666673e-05,
      "loss": 0.2559,
      "step": 11350
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 2.117731809616089,
      "learning_rate": 3.944537037037037e-05,
      "loss": 0.2548,
      "step": 11400
    },
    {
      "epoch": 0.6361111111111111,
      "grad_norm": 1.5447317361831665,
      "learning_rate": 3.939907407407408e-05,
      "loss": 0.2667,
      "step": 11450
    },
    {
      "epoch": 0.6388888888888888,
      "grad_norm": 1.348463773727417,
      "learning_rate": 3.9352777777777776e-05,
      "loss": 0.2756,
      "step": 11500
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 1.472112774848938,
      "learning_rate": 3.930648148148148e-05,
      "loss": 0.227,
      "step": 11550
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 1.2836945056915283,
      "learning_rate": 3.9260185185185187e-05,
      "loss": 0.2424,
      "step": 11600
    },
    {
      "epoch": 0.6472222222222223,
      "grad_norm": 2.352505922317505,
      "learning_rate": 3.921388888888889e-05,
      "loss": 0.2436,
      "step": 11650
    },
    {
      "epoch": 0.65,
      "grad_norm": 1.5375771522521973,
      "learning_rate": 3.91675925925926e-05,
      "loss": 0.2612,
      "step": 11700
    },
    {
      "epoch": 0.6527777777777778,
      "grad_norm": 1.5365252494812012,
      "learning_rate": 3.91212962962963e-05,
      "loss": 0.2549,
      "step": 11750
    },
    {
      "epoch": 0.6555555555555556,
      "grad_norm": 1.2806897163391113,
      "learning_rate": 3.9075e-05,
      "loss": 0.243,
      "step": 11800
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 1.9810032844543457,
      "learning_rate": 3.9028703703703706e-05,
      "loss": 0.2521,
      "step": 11850
    },
    {
      "epoch": 0.6611111111111111,
      "grad_norm": 1.2553951740264893,
      "learning_rate": 3.898240740740741e-05,
      "loss": 0.2732,
      "step": 11900
    },
    {
      "epoch": 0.6638888888888889,
      "grad_norm": 0.5149198174476624,
      "learning_rate": 3.893611111111111e-05,
      "loss": 0.2853,
      "step": 11950
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.7851346731185913,
      "learning_rate": 3.8889814814814816e-05,
      "loss": 0.2495,
      "step": 12000
    },
    {
      "epoch": 0.6694444444444444,
      "grad_norm": 0.9272799491882324,
      "learning_rate": 3.884351851851852e-05,
      "loss": 0.2788,
      "step": 12050
    },
    {
      "epoch": 0.6722222222222223,
      "grad_norm": 1.2525376081466675,
      "learning_rate": 3.8797222222222226e-05,
      "loss": 0.2758,
      "step": 12100
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.6674343347549438,
      "learning_rate": 3.875092592592593e-05,
      "loss": 0.2768,
      "step": 12150
    },
    {
      "epoch": 0.6777777777777778,
      "grad_norm": 1.3434789180755615,
      "learning_rate": 3.870462962962963e-05,
      "loss": 0.233,
      "step": 12200
    },
    {
      "epoch": 0.6805555555555556,
      "grad_norm": 1.6093894243240356,
      "learning_rate": 3.8658333333333336e-05,
      "loss": 0.272,
      "step": 12250
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 1.5129550695419312,
      "learning_rate": 3.861203703703704e-05,
      "loss": 0.2473,
      "step": 12300
    },
    {
      "epoch": 0.6861111111111111,
      "grad_norm": 2.62050724029541,
      "learning_rate": 3.856574074074074e-05,
      "loss": 0.2445,
      "step": 12350
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 6.7095232009887695,
      "learning_rate": 3.8519444444444445e-05,
      "loss": 0.2631,
      "step": 12400
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 1.1470403671264648,
      "learning_rate": 3.847314814814815e-05,
      "loss": 0.252,
      "step": 12450
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 1.8716922998428345,
      "learning_rate": 3.8426851851851856e-05,
      "loss": 0.2403,
      "step": 12500
    },
    {
      "epoch": 0.6972222222222222,
      "grad_norm": 1.4838167428970337,
      "learning_rate": 3.838055555555556e-05,
      "loss": 0.2545,
      "step": 12550
    },
    {
      "epoch": 0.7,
      "grad_norm": 2.723919153213501,
      "learning_rate": 3.833425925925926e-05,
      "loss": 0.2874,
      "step": 12600
    },
    {
      "epoch": 0.7027777777777777,
      "grad_norm": 4.575533866882324,
      "learning_rate": 3.8287962962962965e-05,
      "loss": 0.2585,
      "step": 12650
    },
    {
      "epoch": 0.7055555555555556,
      "grad_norm": 1.9209145307540894,
      "learning_rate": 3.824166666666667e-05,
      "loss": 0.222,
      "step": 12700
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.9513273239135742,
      "learning_rate": 3.819537037037037e-05,
      "loss": 0.2641,
      "step": 12750
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 2.1797497272491455,
      "learning_rate": 3.8149074074074074e-05,
      "loss": 0.2158,
      "step": 12800
    },
    {
      "epoch": 0.7138888888888889,
      "grad_norm": 3.3611581325531006,
      "learning_rate": 3.810277777777778e-05,
      "loss": 0.2473,
      "step": 12850
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 1.6064739227294922,
      "learning_rate": 3.8056481481481485e-05,
      "loss": 0.2987,
      "step": 12900
    },
    {
      "epoch": 0.7194444444444444,
      "grad_norm": 1.3828619718551636,
      "learning_rate": 3.801018518518519e-05,
      "loss": 0.3007,
      "step": 12950
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 1.3045966625213623,
      "learning_rate": 3.7963888888888895e-05,
      "loss": 0.2381,
      "step": 13000
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.9544001817703247,
      "learning_rate": 3.7917592592592594e-05,
      "loss": 0.2147,
      "step": 13050
    },
    {
      "epoch": 0.7277777777777777,
      "grad_norm": 1.1622625589370728,
      "learning_rate": 3.78712962962963e-05,
      "loss": 0.2393,
      "step": 13100
    },
    {
      "epoch": 0.7305555555555555,
      "grad_norm": 0.5139551758766174,
      "learning_rate": 3.7825e-05,
      "loss": 0.2636,
      "step": 13150
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 3.048314094543457,
      "learning_rate": 3.77787037037037e-05,
      "loss": 0.2323,
      "step": 13200
    },
    {
      "epoch": 0.7361111111111112,
      "grad_norm": 1.1663655042648315,
      "learning_rate": 3.773240740740741e-05,
      "loss": 0.2618,
      "step": 13250
    },
    {
      "epoch": 0.7388888888888889,
      "grad_norm": 1.129205346107483,
      "learning_rate": 3.7686111111111114e-05,
      "loss": 0.1989,
      "step": 13300
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 1.5862089395523071,
      "learning_rate": 3.763981481481482e-05,
      "loss": 0.2615,
      "step": 13350
    },
    {
      "epoch": 0.7444444444444445,
      "grad_norm": 2.375896692276001,
      "learning_rate": 3.7593518518518525e-05,
      "loss": 0.2526,
      "step": 13400
    },
    {
      "epoch": 0.7472222222222222,
      "grad_norm": 1.591975450515747,
      "learning_rate": 3.754722222222222e-05,
      "loss": 0.2592,
      "step": 13450
    },
    {
      "epoch": 0.75,
      "grad_norm": 1.7576895952224731,
      "learning_rate": 3.750092592592593e-05,
      "loss": 0.27,
      "step": 13500
    },
    {
      "epoch": 0.7527777777777778,
      "grad_norm": 4.127856254577637,
      "learning_rate": 3.7454629629629634e-05,
      "loss": 0.2767,
      "step": 13550
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 1.5681437253952026,
      "learning_rate": 3.740833333333333e-05,
      "loss": 0.2494,
      "step": 13600
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 1.2680774927139282,
      "learning_rate": 3.736203703703704e-05,
      "loss": 0.2587,
      "step": 13650
    },
    {
      "epoch": 0.7611111111111111,
      "grad_norm": 1.5923022031784058,
      "learning_rate": 3.731574074074074e-05,
      "loss": 0.2609,
      "step": 13700
    },
    {
      "epoch": 0.7638888888888888,
      "grad_norm": 0.9552236199378967,
      "learning_rate": 3.726944444444445e-05,
      "loss": 0.2393,
      "step": 13750
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 1.3317124843597412,
      "learning_rate": 3.7223148148148154e-05,
      "loss": 0.2214,
      "step": 13800
    },
    {
      "epoch": 0.7694444444444445,
      "grad_norm": 2.162529468536377,
      "learning_rate": 3.717685185185185e-05,
      "loss": 0.2059,
      "step": 13850
    },
    {
      "epoch": 0.7722222222222223,
      "grad_norm": 1.5374157428741455,
      "learning_rate": 3.713055555555556e-05,
      "loss": 0.2201,
      "step": 13900
    },
    {
      "epoch": 0.775,
      "grad_norm": 1.299026370048523,
      "learning_rate": 3.708425925925926e-05,
      "loss": 0.2474,
      "step": 13950
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 1.7548859119415283,
      "learning_rate": 3.703796296296296e-05,
      "loss": 0.2771,
      "step": 14000
    },
    {
      "epoch": 0.7805555555555556,
      "grad_norm": 0.7477262616157532,
      "learning_rate": 3.699166666666667e-05,
      "loss": 0.1896,
      "step": 14050
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 1.7525006532669067,
      "learning_rate": 3.694537037037037e-05,
      "loss": 0.2739,
      "step": 14100
    },
    {
      "epoch": 0.7861111111111111,
      "grad_norm": 1.4988774061203003,
      "learning_rate": 3.689907407407408e-05,
      "loss": 0.2334,
      "step": 14150
    },
    {
      "epoch": 0.7888888888888889,
      "grad_norm": 2.416795015335083,
      "learning_rate": 3.685277777777778e-05,
      "loss": 0.2959,
      "step": 14200
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 1.1221773624420166,
      "learning_rate": 3.680648148148148e-05,
      "loss": 0.2548,
      "step": 14250
    },
    {
      "epoch": 0.7944444444444444,
      "grad_norm": 2.5924811363220215,
      "learning_rate": 3.676018518518519e-05,
      "loss": 0.208,
      "step": 14300
    },
    {
      "epoch": 0.7972222222222223,
      "grad_norm": 2.6098921298980713,
      "learning_rate": 3.671388888888889e-05,
      "loss": 0.2679,
      "step": 14350
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.8939170241355896,
      "learning_rate": 3.666759259259259e-05,
      "loss": 0.2428,
      "step": 14400
    },
    {
      "epoch": 0.8027777777777778,
      "grad_norm": 2.861694574356079,
      "learning_rate": 3.6621296296296296e-05,
      "loss": 0.2809,
      "step": 14450
    },
    {
      "epoch": 0.8055555555555556,
      "grad_norm": 1.7441563606262207,
      "learning_rate": 3.6575e-05,
      "loss": 0.2666,
      "step": 14500
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 1.3676825761795044,
      "learning_rate": 3.652870370370371e-05,
      "loss": 0.2526,
      "step": 14550
    },
    {
      "epoch": 0.8111111111111111,
      "grad_norm": 1.8017635345458984,
      "learning_rate": 3.648240740740741e-05,
      "loss": 0.2472,
      "step": 14600
    },
    {
      "epoch": 0.8138888888888889,
      "grad_norm": 1.4146196842193604,
      "learning_rate": 3.643611111111112e-05,
      "loss": 0.265,
      "step": 14650
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 1.3843001127243042,
      "learning_rate": 3.6389814814814816e-05,
      "loss": 0.2389,
      "step": 14700
    },
    {
      "epoch": 0.8194444444444444,
      "grad_norm": 0.6211512684822083,
      "learning_rate": 3.634351851851852e-05,
      "loss": 0.2596,
      "step": 14750
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 0.6700498461723328,
      "learning_rate": 3.629722222222222e-05,
      "loss": 0.2755,
      "step": 14800
    },
    {
      "epoch": 0.825,
      "grad_norm": 2.8348710536956787,
      "learning_rate": 3.6250925925925925e-05,
      "loss": 0.237,
      "step": 14850
    },
    {
      "epoch": 0.8277777777777777,
      "grad_norm": 2.4819767475128174,
      "learning_rate": 3.620462962962963e-05,
      "loss": 0.2325,
      "step": 14900
    },
    {
      "epoch": 0.8305555555555556,
      "grad_norm": 1.8421744108200073,
      "learning_rate": 3.6158333333333336e-05,
      "loss": 0.2661,
      "step": 14950
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 1.1007132530212402,
      "learning_rate": 3.611203703703704e-05,
      "loss": 0.223,
      "step": 15000
    },
    {
      "epoch": 0.8361111111111111,
      "grad_norm": 2.4874775409698486,
      "learning_rate": 3.6065740740740747e-05,
      "loss": 0.2219,
      "step": 15050
    },
    {
      "epoch": 0.8388888888888889,
      "grad_norm": 1.6028143167495728,
      "learning_rate": 3.6019444444444445e-05,
      "loss": 0.2696,
      "step": 15100
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 0.36502426862716675,
      "learning_rate": 3.597314814814815e-05,
      "loss": 0.2422,
      "step": 15150
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.5181393027305603,
      "learning_rate": 3.5926851851851856e-05,
      "loss": 0.2038,
      "step": 15200
    },
    {
      "epoch": 0.8472222222222222,
      "grad_norm": 1.016174554824829,
      "learning_rate": 3.5880555555555554e-05,
      "loss": 0.2793,
      "step": 15250
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.8505008220672607,
      "learning_rate": 3.583425925925926e-05,
      "loss": 0.2303,
      "step": 15300
    },
    {
      "epoch": 0.8527777777777777,
      "grad_norm": 1.183167815208435,
      "learning_rate": 3.5787962962962965e-05,
      "loss": 0.2066,
      "step": 15350
    },
    {
      "epoch": 0.8555555555555555,
      "grad_norm": 1.2174640893936157,
      "learning_rate": 3.574166666666667e-05,
      "loss": 0.2374,
      "step": 15400
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 1.0965622663497925,
      "learning_rate": 3.5695370370370376e-05,
      "loss": 0.2354,
      "step": 15450
    },
    {
      "epoch": 0.8611111111111112,
      "grad_norm": 1.4510564804077148,
      "learning_rate": 3.5649074074074074e-05,
      "loss": 0.2704,
      "step": 15500
    },
    {
      "epoch": 0.8638888888888889,
      "grad_norm": 0.9421354532241821,
      "learning_rate": 3.560277777777778e-05,
      "loss": 0.2297,
      "step": 15550
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 2.1425700187683105,
      "learning_rate": 3.5556481481481485e-05,
      "loss": 0.2428,
      "step": 15600
    },
    {
      "epoch": 0.8694444444444445,
      "grad_norm": 1.2203491926193237,
      "learning_rate": 3.5510185185185183e-05,
      "loss": 0.239,
      "step": 15650
    },
    {
      "epoch": 0.8722222222222222,
      "grad_norm": 2.931081771850586,
      "learning_rate": 3.546388888888889e-05,
      "loss": 0.2377,
      "step": 15700
    },
    {
      "epoch": 0.875,
      "grad_norm": 2.106980800628662,
      "learning_rate": 3.5417592592592594e-05,
      "loss": 0.2467,
      "step": 15750
    },
    {
      "epoch": 0.8777777777777778,
      "grad_norm": 1.334313154220581,
      "learning_rate": 3.53712962962963e-05,
      "loss": 0.2219,
      "step": 15800
    },
    {
      "epoch": 0.8805555555555555,
      "grad_norm": 1.3119254112243652,
      "learning_rate": 3.5325000000000005e-05,
      "loss": 0.3002,
      "step": 15850
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 4.0520548820495605,
      "learning_rate": 3.5278703703703703e-05,
      "loss": 0.2662,
      "step": 15900
    },
    {
      "epoch": 0.8861111111111111,
      "grad_norm": 1.9945601224899292,
      "learning_rate": 3.523240740740741e-05,
      "loss": 0.2587,
      "step": 15950
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.8902921676635742,
      "learning_rate": 3.5186111111111114e-05,
      "loss": 0.2461,
      "step": 16000
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 2.3795135021209717,
      "learning_rate": 3.513981481481481e-05,
      "loss": 0.2067,
      "step": 16050
    },
    {
      "epoch": 0.8944444444444445,
      "grad_norm": 1.2054667472839355,
      "learning_rate": 3.509351851851852e-05,
      "loss": 0.2711,
      "step": 16100
    },
    {
      "epoch": 0.8972222222222223,
      "grad_norm": 0.8065982460975647,
      "learning_rate": 3.504722222222222e-05,
      "loss": 0.1912,
      "step": 16150
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.6184539794921875,
      "learning_rate": 3.500092592592593e-05,
      "loss": 0.2771,
      "step": 16200
    },
    {
      "epoch": 0.9027777777777778,
      "grad_norm": 1.5647194385528564,
      "learning_rate": 3.4954629629629634e-05,
      "loss": 0.2443,
      "step": 16250
    },
    {
      "epoch": 0.9055555555555556,
      "grad_norm": 1.7962137460708618,
      "learning_rate": 3.490833333333334e-05,
      "loss": 0.254,
      "step": 16300
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 0.8728342056274414,
      "learning_rate": 3.486203703703704e-05,
      "loss": 0.2486,
      "step": 16350
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 1.8506405353546143,
      "learning_rate": 3.481574074074074e-05,
      "loss": 0.2136,
      "step": 16400
    },
    {
      "epoch": 0.9138888888888889,
      "grad_norm": 1.9088190793991089,
      "learning_rate": 3.476944444444444e-05,
      "loss": 0.2603,
      "step": 16450
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.9583960771560669,
      "learning_rate": 3.472314814814815e-05,
      "loss": 0.2078,
      "step": 16500
    },
    {
      "epoch": 0.9194444444444444,
      "grad_norm": 0.686771810054779,
      "learning_rate": 3.467685185185185e-05,
      "loss": 0.2601,
      "step": 16550
    },
    {
      "epoch": 0.9222222222222223,
      "grad_norm": 0.8228297233581543,
      "learning_rate": 3.463055555555556e-05,
      "loss": 0.2175,
      "step": 16600
    },
    {
      "epoch": 0.925,
      "grad_norm": 0.3969675600528717,
      "learning_rate": 3.458425925925926e-05,
      "loss": 0.2152,
      "step": 16650
    },
    {
      "epoch": 0.9277777777777778,
      "grad_norm": 1.1585339307785034,
      "learning_rate": 3.453796296296297e-05,
      "loss": 0.2388,
      "step": 16700
    },
    {
      "epoch": 0.9305555555555556,
      "grad_norm": 1.3657091856002808,
      "learning_rate": 3.449166666666667e-05,
      "loss": 0.2513,
      "step": 16750
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.7683897018432617,
      "learning_rate": 3.444537037037037e-05,
      "loss": 0.2432,
      "step": 16800
    },
    {
      "epoch": 0.9361111111111111,
      "grad_norm": 1.3743623495101929,
      "learning_rate": 3.439907407407408e-05,
      "loss": 0.2676,
      "step": 16850
    },
    {
      "epoch": 0.9388888888888889,
      "grad_norm": 0.6997423768043518,
      "learning_rate": 3.4352777777777776e-05,
      "loss": 0.2164,
      "step": 16900
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 3.5373387336730957,
      "learning_rate": 3.430648148148148e-05,
      "loss": 0.2506,
      "step": 16950
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 2.004011869430542,
      "learning_rate": 3.426018518518519e-05,
      "loss": 0.2839,
      "step": 17000
    },
    {
      "epoch": 0.9472222222222222,
      "grad_norm": 0.8084471225738525,
      "learning_rate": 3.421388888888889e-05,
      "loss": 0.2186,
      "step": 17050
    },
    {
      "epoch": 0.95,
      "grad_norm": 1.3507579565048218,
      "learning_rate": 3.41675925925926e-05,
      "loss": 0.2465,
      "step": 17100
    },
    {
      "epoch": 0.9527777777777777,
      "grad_norm": 1.0217536687850952,
      "learning_rate": 3.4121296296296296e-05,
      "loss": 0.229,
      "step": 17150
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 1.900870442390442,
      "learning_rate": 3.4075e-05,
      "loss": 0.2257,
      "step": 17200
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 1.47752046585083,
      "learning_rate": 3.402870370370371e-05,
      "loss": 0.233,
      "step": 17250
    },
    {
      "epoch": 0.9611111111111111,
      "grad_norm": 0.7193745970726013,
      "learning_rate": 3.3982407407407405e-05,
      "loss": 0.2363,
      "step": 17300
    },
    {
      "epoch": 0.9638888888888889,
      "grad_norm": 1.1733567714691162,
      "learning_rate": 3.393611111111111e-05,
      "loss": 0.2383,
      "step": 17350
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 2.2936999797821045,
      "learning_rate": 3.3889814814814816e-05,
      "loss": 0.2507,
      "step": 17400
    },
    {
      "epoch": 0.9694444444444444,
      "grad_norm": 0.6118925213813782,
      "learning_rate": 3.384351851851852e-05,
      "loss": 0.2574,
      "step": 17450
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 1.0295038223266602,
      "learning_rate": 3.379722222222223e-05,
      "loss": 0.197,
      "step": 17500
    },
    {
      "epoch": 0.975,
      "grad_norm": 1.1638484001159668,
      "learning_rate": 3.375092592592593e-05,
      "loss": 0.2318,
      "step": 17550
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 2.016268730163574,
      "learning_rate": 3.370462962962963e-05,
      "loss": 0.241,
      "step": 17600
    },
    {
      "epoch": 0.9805555555555555,
      "grad_norm": 1.5474625825881958,
      "learning_rate": 3.3658333333333336e-05,
      "loss": 0.2787,
      "step": 17650
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 0.9814411997795105,
      "learning_rate": 3.3612037037037035e-05,
      "loss": 0.2532,
      "step": 17700
    },
    {
      "epoch": 0.9861111111111112,
      "grad_norm": 1.0341486930847168,
      "learning_rate": 3.356574074074074e-05,
      "loss": 0.238,
      "step": 17750
    },
    {
      "epoch": 0.9888888888888889,
      "grad_norm": 1.2672287225723267,
      "learning_rate": 3.3519444444444445e-05,
      "loss": 0.2416,
      "step": 17800
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 2.0352935791015625,
      "learning_rate": 3.347314814814815e-05,
      "loss": 0.2268,
      "step": 17850
    },
    {
      "epoch": 0.9944444444444445,
      "grad_norm": 3.5000932216644287,
      "learning_rate": 3.3426851851851856e-05,
      "loss": 0.2424,
      "step": 17900
    },
    {
      "epoch": 0.9972222222222222,
      "grad_norm": 1.5082758665084839,
      "learning_rate": 3.338055555555556e-05,
      "loss": 0.2264,
      "step": 17950
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.8398927450180054,
      "learning_rate": 3.333425925925926e-05,
      "loss": 0.2278,
      "step": 18000
    },
    {
      "epoch": 1.0027777777777778,
      "grad_norm": 0.6790112257003784,
      "learning_rate": 3.3287962962962965e-05,
      "loss": 0.2301,
      "step": 18050
    },
    {
      "epoch": 1.0055555555555555,
      "grad_norm": 1.9989897012710571,
      "learning_rate": 3.324166666666667e-05,
      "loss": 0.2034,
      "step": 18100
    },
    {
      "epoch": 1.0083333333333333,
      "grad_norm": 1.1865259408950806,
      "learning_rate": 3.319537037037037e-05,
      "loss": 0.2311,
      "step": 18150
    },
    {
      "epoch": 1.011111111111111,
      "grad_norm": 0.848738431930542,
      "learning_rate": 3.3149074074074074e-05,
      "loss": 0.2242,
      "step": 18200
    },
    {
      "epoch": 1.0138888888888888,
      "grad_norm": 1.7646621465682983,
      "learning_rate": 3.310277777777778e-05,
      "loss": 0.2108,
      "step": 18250
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 0.6749803423881531,
      "learning_rate": 3.3056481481481485e-05,
      "loss": 0.2051,
      "step": 18300
    },
    {
      "epoch": 1.0194444444444444,
      "grad_norm": 2.3541598320007324,
      "learning_rate": 3.301018518518519e-05,
      "loss": 0.2283,
      "step": 18350
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 1.7720224857330322,
      "learning_rate": 3.296388888888889e-05,
      "loss": 0.2242,
      "step": 18400
    },
    {
      "epoch": 1.025,
      "grad_norm": 0.9645031690597534,
      "learning_rate": 3.2917592592592594e-05,
      "loss": 0.2288,
      "step": 18450
    },
    {
      "epoch": 1.0277777777777777,
      "grad_norm": 1.7111269235610962,
      "learning_rate": 3.28712962962963e-05,
      "loss": 0.1898,
      "step": 18500
    },
    {
      "epoch": 1.0305555555555554,
      "grad_norm": 1.7880933284759521,
      "learning_rate": 3.2825e-05,
      "loss": 0.2293,
      "step": 18550
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 1.3753045797348022,
      "learning_rate": 3.2778703703703704e-05,
      "loss": 0.1851,
      "step": 18600
    },
    {
      "epoch": 1.0361111111111112,
      "grad_norm": 1.0504553318023682,
      "learning_rate": 3.273240740740741e-05,
      "loss": 0.2053,
      "step": 18650
    },
    {
      "epoch": 1.038888888888889,
      "grad_norm": 0.6944963335990906,
      "learning_rate": 3.2686111111111114e-05,
      "loss": 0.2053,
      "step": 18700
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 1.425808310508728,
      "learning_rate": 3.263981481481482e-05,
      "loss": 0.2029,
      "step": 18750
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 1.6127294301986694,
      "learning_rate": 3.259351851851852e-05,
      "loss": 0.2012,
      "step": 18800
    },
    {
      "epoch": 1.0472222222222223,
      "grad_norm": 1.5975563526153564,
      "learning_rate": 3.2547222222222224e-05,
      "loss": 0.2087,
      "step": 18850
    },
    {
      "epoch": 1.05,
      "grad_norm": 1.5445283651351929,
      "learning_rate": 3.250092592592593e-05,
      "loss": 0.2322,
      "step": 18900
    },
    {
      "epoch": 1.0527777777777778,
      "grad_norm": 0.503599226474762,
      "learning_rate": 3.245462962962963e-05,
      "loss": 0.2152,
      "step": 18950
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 1.0703524351119995,
      "learning_rate": 3.240833333333333e-05,
      "loss": 0.1861,
      "step": 19000
    },
    {
      "epoch": 1.0583333333333333,
      "grad_norm": 1.461976170539856,
      "learning_rate": 3.236203703703704e-05,
      "loss": 0.1907,
      "step": 19050
    },
    {
      "epoch": 1.0611111111111111,
      "grad_norm": 1.793363094329834,
      "learning_rate": 3.2315740740740743e-05,
      "loss": 0.1829,
      "step": 19100
    },
    {
      "epoch": 1.0638888888888889,
      "grad_norm": 0.9289801120758057,
      "learning_rate": 3.226944444444445e-05,
      "loss": 0.2017,
      "step": 19150
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 1.9669862985610962,
      "learning_rate": 3.2223148148148154e-05,
      "loss": 0.2179,
      "step": 19200
    },
    {
      "epoch": 1.0694444444444444,
      "grad_norm": 1.9738538265228271,
      "learning_rate": 3.217685185185185e-05,
      "loss": 0.232,
      "step": 19250
    },
    {
      "epoch": 1.0722222222222222,
      "grad_norm": 0.8349416851997375,
      "learning_rate": 3.213055555555556e-05,
      "loss": 0.2064,
      "step": 19300
    },
    {
      "epoch": 1.075,
      "grad_norm": 0.8217470049858093,
      "learning_rate": 3.2084259259259257e-05,
      "loss": 0.2307,
      "step": 19350
    },
    {
      "epoch": 1.0777777777777777,
      "grad_norm": 0.9831351041793823,
      "learning_rate": 3.203796296296296e-05,
      "loss": 0.2126,
      "step": 19400
    },
    {
      "epoch": 1.0805555555555555,
      "grad_norm": 1.279262900352478,
      "learning_rate": 3.199166666666667e-05,
      "loss": 0.233,
      "step": 19450
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 1.4998940229415894,
      "learning_rate": 3.194537037037037e-05,
      "loss": 0.21,
      "step": 19500
    },
    {
      "epoch": 1.086111111111111,
      "grad_norm": 1.9826444387435913,
      "learning_rate": 3.189907407407408e-05,
      "loss": 0.2081,
      "step": 19550
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 1.2172456979751587,
      "learning_rate": 3.185277777777778e-05,
      "loss": 0.1942,
      "step": 19600
    },
    {
      "epoch": 1.0916666666666666,
      "grad_norm": 1.0028990507125854,
      "learning_rate": 3.180648148148148e-05,
      "loss": 0.1749,
      "step": 19650
    },
    {
      "epoch": 1.0944444444444446,
      "grad_norm": 1.8055589199066162,
      "learning_rate": 3.176018518518519e-05,
      "loss": 0.2338,
      "step": 19700
    },
    {
      "epoch": 1.0972222222222223,
      "grad_norm": 2.87422776222229,
      "learning_rate": 3.171388888888889e-05,
      "loss": 0.199,
      "step": 19750
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.7616974711418152,
      "learning_rate": 3.166759259259259e-05,
      "loss": 0.1889,
      "step": 19800
    },
    {
      "epoch": 1.1027777777777779,
      "grad_norm": 1.1944366693496704,
      "learning_rate": 3.1621296296296296e-05,
      "loss": 0.225,
      "step": 19850
    },
    {
      "epoch": 1.1055555555555556,
      "grad_norm": 0.2487652748823166,
      "learning_rate": 3.1575e-05,
      "loss": 0.2052,
      "step": 19900
    },
    {
      "epoch": 1.1083333333333334,
      "grad_norm": 2.3381569385528564,
      "learning_rate": 3.152870370370371e-05,
      "loss": 0.2242,
      "step": 19950
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 1.2318083047866821,
      "learning_rate": 3.148240740740741e-05,
      "loss": 0.2052,
      "step": 20000
    },
    {
      "epoch": 1.113888888888889,
      "grad_norm": 0.4354992210865021,
      "learning_rate": 3.143611111111111e-05,
      "loss": 0.2096,
      "step": 20050
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 0.6215895414352417,
      "learning_rate": 3.1389814814814816e-05,
      "loss": 0.188,
      "step": 20100
    },
    {
      "epoch": 1.1194444444444445,
      "grad_norm": 0.8003014326095581,
      "learning_rate": 3.134351851851852e-05,
      "loss": 0.2388,
      "step": 20150
    },
    {
      "epoch": 1.1222222222222222,
      "grad_norm": 1.8325239419937134,
      "learning_rate": 3.129722222222222e-05,
      "loss": 0.2185,
      "step": 20200
    },
    {
      "epoch": 1.125,
      "grad_norm": 2.216035842895508,
      "learning_rate": 3.1250925925925926e-05,
      "loss": 0.2066,
      "step": 20250
    },
    {
      "epoch": 1.1277777777777778,
      "grad_norm": 0.8227348923683167,
      "learning_rate": 3.120462962962963e-05,
      "loss": 0.2069,
      "step": 20300
    },
    {
      "epoch": 1.1305555555555555,
      "grad_norm": 1.6893497705459595,
      "learning_rate": 3.1158333333333336e-05,
      "loss": 0.2015,
      "step": 20350
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 1.588793396949768,
      "learning_rate": 3.111203703703704e-05,
      "loss": 0.1934,
      "step": 20400
    },
    {
      "epoch": 1.136111111111111,
      "grad_norm": 2.075030565261841,
      "learning_rate": 3.106574074074074e-05,
      "loss": 0.201,
      "step": 20450
    },
    {
      "epoch": 1.1388888888888888,
      "grad_norm": 0.555181086063385,
      "learning_rate": 3.1019444444444446e-05,
      "loss": 0.1962,
      "step": 20500
    },
    {
      "epoch": 1.1416666666666666,
      "grad_norm": 3.860440969467163,
      "learning_rate": 3.097314814814815e-05,
      "loss": 0.2156,
      "step": 20550
    },
    {
      "epoch": 1.1444444444444444,
      "grad_norm": 0.9213438034057617,
      "learning_rate": 3.092685185185185e-05,
      "loss": 0.2212,
      "step": 20600
    },
    {
      "epoch": 1.1472222222222221,
      "grad_norm": 0.8669041991233826,
      "learning_rate": 3.0880555555555555e-05,
      "loss": 0.1851,
      "step": 20650
    },
    {
      "epoch": 1.15,
      "grad_norm": 1.1422853469848633,
      "learning_rate": 3.083425925925926e-05,
      "loss": 0.1778,
      "step": 20700
    },
    {
      "epoch": 1.1527777777777777,
      "grad_norm": 1.6509013175964355,
      "learning_rate": 3.0787962962962965e-05,
      "loss": 0.2405,
      "step": 20750
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 0.9726919531822205,
      "learning_rate": 3.074166666666667e-05,
      "loss": 0.2204,
      "step": 20800
    },
    {
      "epoch": 1.1583333333333332,
      "grad_norm": 0.8402203321456909,
      "learning_rate": 3.0695370370370376e-05,
      "loss": 0.2236,
      "step": 20850
    },
    {
      "epoch": 1.1611111111111112,
      "grad_norm": 1.096203088760376,
      "learning_rate": 3.0649074074074075e-05,
      "loss": 0.2205,
      "step": 20900
    },
    {
      "epoch": 1.163888888888889,
      "grad_norm": 1.0206550359725952,
      "learning_rate": 3.060277777777778e-05,
      "loss": 0.2052,
      "step": 20950
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 2.318267345428467,
      "learning_rate": 3.055648148148148e-05,
      "loss": 0.2331,
      "step": 21000
    },
    {
      "epoch": 1.1694444444444445,
      "grad_norm": 0.7558466792106628,
      "learning_rate": 3.0510185185185187e-05,
      "loss": 0.1838,
      "step": 21050
    },
    {
      "epoch": 1.1722222222222223,
      "grad_norm": 2.589393377304077,
      "learning_rate": 3.0463888888888893e-05,
      "loss": 0.2213,
      "step": 21100
    },
    {
      "epoch": 1.175,
      "grad_norm": 1.7651598453521729,
      "learning_rate": 3.041759259259259e-05,
      "loss": 0.2332,
      "step": 21150
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 0.7591058015823364,
      "learning_rate": 3.0371296296296297e-05,
      "loss": 0.2058,
      "step": 21200
    },
    {
      "epoch": 1.1805555555555556,
      "grad_norm": 0.7526049613952637,
      "learning_rate": 3.0325000000000002e-05,
      "loss": 0.2235,
      "step": 21250
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 1.6143553256988525,
      "learning_rate": 3.0278703703703704e-05,
      "loss": 0.1908,
      "step": 21300
    },
    {
      "epoch": 1.1861111111111111,
      "grad_norm": 0.8097288012504578,
      "learning_rate": 3.023240740740741e-05,
      "loss": 0.185,
      "step": 21350
    },
    {
      "epoch": 1.1888888888888889,
      "grad_norm": 1.73171067237854,
      "learning_rate": 3.0186111111111115e-05,
      "loss": 0.1652,
      "step": 21400
    },
    {
      "epoch": 1.1916666666666667,
      "grad_norm": 1.1969852447509766,
      "learning_rate": 3.0139814814814816e-05,
      "loss": 0.1946,
      "step": 21450
    },
    {
      "epoch": 1.1944444444444444,
      "grad_norm": 2.030146598815918,
      "learning_rate": 3.0093518518518522e-05,
      "loss": 0.2505,
      "step": 21500
    },
    {
      "epoch": 1.1972222222222222,
      "grad_norm": 1.980370044708252,
      "learning_rate": 3.004722222222222e-05,
      "loss": 0.217,
      "step": 21550
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.6034877300262451,
      "learning_rate": 3.0000925925925926e-05,
      "loss": 0.2091,
      "step": 21600
    },
    {
      "epoch": 1.2027777777777777,
      "grad_norm": 0.5512682795524597,
      "learning_rate": 2.995462962962963e-05,
      "loss": 0.1861,
      "step": 21650
    },
    {
      "epoch": 1.2055555555555555,
      "grad_norm": 0.8415317535400391,
      "learning_rate": 2.9908333333333333e-05,
      "loss": 0.1929,
      "step": 21700
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 2.8267226219177246,
      "learning_rate": 2.986203703703704e-05,
      "loss": 0.2073,
      "step": 21750
    },
    {
      "epoch": 1.211111111111111,
      "grad_norm": 1.517513632774353,
      "learning_rate": 2.9815740740740744e-05,
      "loss": 0.2003,
      "step": 21800
    },
    {
      "epoch": 1.2138888888888888,
      "grad_norm": 1.4139567613601685,
      "learning_rate": 2.9769444444444446e-05,
      "loss": 0.2311,
      "step": 21850
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 1.5092294216156006,
      "learning_rate": 2.972314814814815e-05,
      "loss": 0.1874,
      "step": 21900
    },
    {
      "epoch": 1.2194444444444446,
      "grad_norm": 1.9930623769760132,
      "learning_rate": 2.9676851851851856e-05,
      "loss": 0.1923,
      "step": 21950
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 3.8731517791748047,
      "learning_rate": 2.9630555555555555e-05,
      "loss": 0.1904,
      "step": 22000
    },
    {
      "epoch": 1.225,
      "grad_norm": 2.801630973815918,
      "learning_rate": 2.958425925925926e-05,
      "loss": 0.1911,
      "step": 22050
    },
    {
      "epoch": 1.2277777777777779,
      "grad_norm": 0.734890341758728,
      "learning_rate": 2.9537962962962962e-05,
      "loss": 0.1809,
      "step": 22100
    },
    {
      "epoch": 1.2305555555555556,
      "grad_norm": 0.846753716468811,
      "learning_rate": 2.9491666666666667e-05,
      "loss": 0.1923,
      "step": 22150
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 1.6034631729125977,
      "learning_rate": 2.9445370370370373e-05,
      "loss": 0.2272,
      "step": 22200
    },
    {
      "epoch": 1.2361111111111112,
      "grad_norm": 1.8282326459884644,
      "learning_rate": 2.9399074074074075e-05,
      "loss": 0.2098,
      "step": 22250
    },
    {
      "epoch": 1.238888888888889,
      "grad_norm": 0.45737069845199585,
      "learning_rate": 2.935277777777778e-05,
      "loss": 0.204,
      "step": 22300
    },
    {
      "epoch": 1.2416666666666667,
      "grad_norm": 1.488303303718567,
      "learning_rate": 2.9306481481481485e-05,
      "loss": 0.2455,
      "step": 22350
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 2.574373245239258,
      "learning_rate": 2.9260185185185184e-05,
      "loss": 0.2275,
      "step": 22400
    },
    {
      "epoch": 1.2472222222222222,
      "grad_norm": 0.9769026637077332,
      "learning_rate": 2.921388888888889e-05,
      "loss": 0.1932,
      "step": 22450
    },
    {
      "epoch": 1.25,
      "grad_norm": 2.7784528732299805,
      "learning_rate": 2.9167592592592595e-05,
      "loss": 0.2093,
      "step": 22500
    },
    {
      "epoch": 1.2527777777777778,
      "grad_norm": 1.0394527912139893,
      "learning_rate": 2.9121296296296297e-05,
      "loss": 0.1833,
      "step": 22550
    },
    {
      "epoch": 1.2555555555555555,
      "grad_norm": 0.901029646396637,
      "learning_rate": 2.9075000000000002e-05,
      "loss": 0.2067,
      "step": 22600
    },
    {
      "epoch": 1.2583333333333333,
      "grad_norm": 0.6816725730895996,
      "learning_rate": 2.9028703703703704e-05,
      "loss": 0.1892,
      "step": 22650
    },
    {
      "epoch": 1.261111111111111,
      "grad_norm": 0.48860621452331543,
      "learning_rate": 2.898240740740741e-05,
      "loss": 0.2013,
      "step": 22700
    },
    {
      "epoch": 1.2638888888888888,
      "grad_norm": 2.6835062503814697,
      "learning_rate": 2.8936111111111115e-05,
      "loss": 0.1946,
      "step": 22750
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 1.4066803455352783,
      "learning_rate": 2.8889814814814813e-05,
      "loss": 0.2521,
      "step": 22800
    },
    {
      "epoch": 1.2694444444444444,
      "grad_norm": 1.0418598651885986,
      "learning_rate": 2.884351851851852e-05,
      "loss": 0.2284,
      "step": 22850
    },
    {
      "epoch": 1.2722222222222221,
      "grad_norm": 0.3859791159629822,
      "learning_rate": 2.8797222222222224e-05,
      "loss": 0.1856,
      "step": 22900
    },
    {
      "epoch": 1.275,
      "grad_norm": 0.4219968914985657,
      "learning_rate": 2.8750925925925926e-05,
      "loss": 0.2128,
      "step": 22950
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 1.4346587657928467,
      "learning_rate": 2.870462962962963e-05,
      "loss": 0.2023,
      "step": 23000
    },
    {
      "epoch": 1.2805555555555554,
      "grad_norm": 1.7797389030456543,
      "learning_rate": 2.8658333333333336e-05,
      "loss": 0.2159,
      "step": 23050
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 0.8320563435554504,
      "learning_rate": 2.861203703703704e-05,
      "loss": 0.2044,
      "step": 23100
    },
    {
      "epoch": 1.286111111111111,
      "grad_norm": 1.5931893587112427,
      "learning_rate": 2.8565740740740744e-05,
      "loss": 0.2476,
      "step": 23150
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 1.2129255533218384,
      "learning_rate": 2.8519444444444442e-05,
      "loss": 0.1731,
      "step": 23200
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 1.8357211351394653,
      "learning_rate": 2.8473148148148148e-05,
      "loss": 0.2063,
      "step": 23250
    },
    {
      "epoch": 1.2944444444444445,
      "grad_norm": 1.1853572130203247,
      "learning_rate": 2.8426851851851853e-05,
      "loss": 0.1797,
      "step": 23300
    },
    {
      "epoch": 1.2972222222222223,
      "grad_norm": 1.726877212524414,
      "learning_rate": 2.8380555555555555e-05,
      "loss": 0.1826,
      "step": 23350
    },
    {
      "epoch": 1.3,
      "grad_norm": 2.7953383922576904,
      "learning_rate": 2.833425925925926e-05,
      "loss": 0.1996,
      "step": 23400
    },
    {
      "epoch": 1.3027777777777778,
      "grad_norm": 1.517143726348877,
      "learning_rate": 2.8287962962962966e-05,
      "loss": 0.1598,
      "step": 23450
    },
    {
      "epoch": 1.3055555555555556,
      "grad_norm": 1.5423948764801025,
      "learning_rate": 2.8241666666666668e-05,
      "loss": 0.2118,
      "step": 23500
    },
    {
      "epoch": 1.3083333333333333,
      "grad_norm": 3.010310411453247,
      "learning_rate": 2.8195370370370373e-05,
      "loss": 0.1715,
      "step": 23550
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 1.203911304473877,
      "learning_rate": 2.8149074074074078e-05,
      "loss": 0.1873,
      "step": 23600
    },
    {
      "epoch": 1.3138888888888889,
      "grad_norm": 4.202988624572754,
      "learning_rate": 2.8102777777777777e-05,
      "loss": 0.1736,
      "step": 23650
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 1.5427639484405518,
      "learning_rate": 2.8056481481481482e-05,
      "loss": 0.1949,
      "step": 23700
    },
    {
      "epoch": 1.3194444444444444,
      "grad_norm": 0.7146926522254944,
      "learning_rate": 2.8010185185185184e-05,
      "loss": 0.1646,
      "step": 23750
    },
    {
      "epoch": 1.3222222222222222,
      "grad_norm": 2.0664563179016113,
      "learning_rate": 2.796388888888889e-05,
      "loss": 0.2118,
      "step": 23800
    },
    {
      "epoch": 1.325,
      "grad_norm": 0.7559933662414551,
      "learning_rate": 2.7917592592592595e-05,
      "loss": 0.2112,
      "step": 23850
    },
    {
      "epoch": 1.3277777777777777,
      "grad_norm": 0.6908349394798279,
      "learning_rate": 2.7871296296296297e-05,
      "loss": 0.1878,
      "step": 23900
    },
    {
      "epoch": 1.3305555555555555,
      "grad_norm": 0.9053505063056946,
      "learning_rate": 2.7825000000000002e-05,
      "loss": 0.1864,
      "step": 23950
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.30911868810653687,
      "learning_rate": 2.7778703703703707e-05,
      "loss": 0.2158,
      "step": 24000
    },
    {
      "epoch": 1.3361111111111112,
      "grad_norm": 1.2394673824310303,
      "learning_rate": 2.7732407407407406e-05,
      "loss": 0.1778,
      "step": 24050
    },
    {
      "epoch": 1.338888888888889,
      "grad_norm": 1.006468653678894,
      "learning_rate": 2.768611111111111e-05,
      "loss": 0.1916,
      "step": 24100
    },
    {
      "epoch": 1.3416666666666668,
      "grad_norm": 1.5664461851119995,
      "learning_rate": 2.763981481481482e-05,
      "loss": 0.2057,
      "step": 24150
    },
    {
      "epoch": 1.3444444444444446,
      "grad_norm": 0.9230061173439026,
      "learning_rate": 2.759351851851852e-05,
      "loss": 0.1869,
      "step": 24200
    },
    {
      "epoch": 1.3472222222222223,
      "grad_norm": 0.9912927746772766,
      "learning_rate": 2.7547222222222224e-05,
      "loss": 0.2029,
      "step": 24250
    },
    {
      "epoch": 1.35,
      "grad_norm": 1.1078294515609741,
      "learning_rate": 2.750092592592593e-05,
      "loss": 0.2016,
      "step": 24300
    },
    {
      "epoch": 1.3527777777777779,
      "grad_norm": 0.7113861441612244,
      "learning_rate": 2.745462962962963e-05,
      "loss": 0.1974,
      "step": 24350
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 1.4069424867630005,
      "learning_rate": 2.7408333333333337e-05,
      "loss": 0.1612,
      "step": 24400
    },
    {
      "epoch": 1.3583333333333334,
      "grad_norm": 0.8424291014671326,
      "learning_rate": 2.7362037037037035e-05,
      "loss": 0.2044,
      "step": 24450
    },
    {
      "epoch": 1.3611111111111112,
      "grad_norm": 1.1988884210586548,
      "learning_rate": 2.731574074074074e-05,
      "loss": 0.1908,
      "step": 24500
    },
    {
      "epoch": 1.363888888888889,
      "grad_norm": 1.04678475856781,
      "learning_rate": 2.7269444444444446e-05,
      "loss": 0.2306,
      "step": 24550
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 1.00271737575531,
      "learning_rate": 2.7223148148148148e-05,
      "loss": 0.1893,
      "step": 24600
    },
    {
      "epoch": 1.3694444444444445,
      "grad_norm": 1.6702696084976196,
      "learning_rate": 2.7176851851851853e-05,
      "loss": 0.2231,
      "step": 24650
    },
    {
      "epoch": 1.3722222222222222,
      "grad_norm": 0.757074773311615,
      "learning_rate": 2.713055555555556e-05,
      "loss": 0.1653,
      "step": 24700
    },
    {
      "epoch": 1.375,
      "grad_norm": 1.8198282718658447,
      "learning_rate": 2.708425925925926e-05,
      "loss": 0.1978,
      "step": 24750
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 0.5993550419807434,
      "learning_rate": 2.7037962962962966e-05,
      "loss": 0.2362,
      "step": 24800
    },
    {
      "epoch": 1.3805555555555555,
      "grad_norm": 2.3387162685394287,
      "learning_rate": 2.699166666666667e-05,
      "loss": 0.1695,
      "step": 24850
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 2.56257963180542,
      "learning_rate": 2.694537037037037e-05,
      "loss": 0.1947,
      "step": 24900
    },
    {
      "epoch": 1.386111111111111,
      "grad_norm": 0.511946976184845,
      "learning_rate": 2.6899074074074075e-05,
      "loss": 0.212,
      "step": 24950
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 1.5659908056259155,
      "learning_rate": 2.6852777777777777e-05,
      "loss": 0.1821,
      "step": 25000
    },
    {
      "epoch": 1.3916666666666666,
      "grad_norm": 1.7277555465698242,
      "learning_rate": 2.6806481481481482e-05,
      "loss": 0.2211,
      "step": 25050
    },
    {
      "epoch": 1.3944444444444444,
      "grad_norm": 3.125434160232544,
      "learning_rate": 2.6760185185185188e-05,
      "loss": 0.2176,
      "step": 25100
    },
    {
      "epoch": 1.3972222222222221,
      "grad_norm": 1.1311161518096924,
      "learning_rate": 2.671388888888889e-05,
      "loss": 0.1852,
      "step": 25150
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.029655933380127,
      "learning_rate": 2.6667592592592595e-05,
      "loss": 0.197,
      "step": 25200
    },
    {
      "epoch": 1.4027777777777777,
      "grad_norm": 1.4749858379364014,
      "learning_rate": 2.66212962962963e-05,
      "loss": 0.1809,
      "step": 25250
    },
    {
      "epoch": 1.4055555555555554,
      "grad_norm": 1.0194320678710938,
      "learning_rate": 2.6575e-05,
      "loss": 0.2351,
      "step": 25300
    },
    {
      "epoch": 1.4083333333333332,
      "grad_norm": 1.475429654121399,
      "learning_rate": 2.6528703703703704e-05,
      "loss": 0.2313,
      "step": 25350
    },
    {
      "epoch": 1.411111111111111,
      "grad_norm": 0.9973841309547424,
      "learning_rate": 2.6482407407407413e-05,
      "loss": 0.2265,
      "step": 25400
    },
    {
      "epoch": 1.4138888888888888,
      "grad_norm": 1.1298167705535889,
      "learning_rate": 2.643611111111111e-05,
      "loss": 0.175,
      "step": 25450
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.6654495000839233,
      "learning_rate": 2.6389814814814817e-05,
      "loss": 0.1996,
      "step": 25500
    },
    {
      "epoch": 1.4194444444444445,
      "grad_norm": 0.7687496542930603,
      "learning_rate": 2.634351851851852e-05,
      "loss": 0.1828,
      "step": 25550
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 0.8750690221786499,
      "learning_rate": 2.6297222222222224e-05,
      "loss": 0.2027,
      "step": 25600
    },
    {
      "epoch": 1.425,
      "grad_norm": 0.9505432844161987,
      "learning_rate": 2.625092592592593e-05,
      "loss": 0.2058,
      "step": 25650
    },
    {
      "epoch": 1.4277777777777778,
      "grad_norm": 1.2020158767700195,
      "learning_rate": 2.6204629629629628e-05,
      "loss": 0.2074,
      "step": 25700
    },
    {
      "epoch": 1.4305555555555556,
      "grad_norm": 1.3100861310958862,
      "learning_rate": 2.6158333333333333e-05,
      "loss": 0.2342,
      "step": 25750
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.520115852355957,
      "learning_rate": 2.6112037037037042e-05,
      "loss": 0.1867,
      "step": 25800
    },
    {
      "epoch": 1.4361111111111111,
      "grad_norm": 1.4349757432937622,
      "learning_rate": 2.606574074074074e-05,
      "loss": 0.1978,
      "step": 25850
    },
    {
      "epoch": 1.4388888888888889,
      "grad_norm": 0.8485414385795593,
      "learning_rate": 2.6019444444444446e-05,
      "loss": 0.1887,
      "step": 25900
    },
    {
      "epoch": 1.4416666666666667,
      "grad_norm": 0.968183696269989,
      "learning_rate": 2.597314814814815e-05,
      "loss": 0.2068,
      "step": 25950
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 2.8566246032714844,
      "learning_rate": 2.5926851851851853e-05,
      "loss": 0.1769,
      "step": 26000
    },
    {
      "epoch": 1.4472222222222222,
      "grad_norm": 1.4011558294296265,
      "learning_rate": 2.588055555555556e-05,
      "loss": 0.1752,
      "step": 26050
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.23116600513458252,
      "learning_rate": 2.5834259259259257e-05,
      "loss": 0.1663,
      "step": 26100
    },
    {
      "epoch": 1.4527777777777777,
      "grad_norm": 0.9095635414123535,
      "learning_rate": 2.5787962962962962e-05,
      "loss": 0.2171,
      "step": 26150
    },
    {
      "epoch": 1.4555555555555555,
      "grad_norm": 0.26753950119018555,
      "learning_rate": 2.5741666666666668e-05,
      "loss": 0.2145,
      "step": 26200
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 2.0257842540740967,
      "learning_rate": 2.569537037037037e-05,
      "loss": 0.2091,
      "step": 26250
    },
    {
      "epoch": 1.4611111111111112,
      "grad_norm": 1.5386492013931274,
      "learning_rate": 2.5649074074074075e-05,
      "loss": 0.2025,
      "step": 26300
    },
    {
      "epoch": 1.463888888888889,
      "grad_norm": 5.048641204833984,
      "learning_rate": 2.560277777777778e-05,
      "loss": 0.1918,
      "step": 26350
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.9421324133872986,
      "learning_rate": 2.5556481481481482e-05,
      "loss": 0.2182,
      "step": 26400
    },
    {
      "epoch": 1.4694444444444446,
      "grad_norm": 0.8337903618812561,
      "learning_rate": 2.5510185185185188e-05,
      "loss": 0.1761,
      "step": 26450
    },
    {
      "epoch": 1.4722222222222223,
      "grad_norm": 0.7336677312850952,
      "learning_rate": 2.5463888888888893e-05,
      "loss": 0.1892,
      "step": 26500
    },
    {
      "epoch": 1.475,
      "grad_norm": 1.78253972530365,
      "learning_rate": 2.541759259259259e-05,
      "loss": 0.1692,
      "step": 26550
    },
    {
      "epoch": 1.4777777777777779,
      "grad_norm": 0.5486238598823547,
      "learning_rate": 2.5371296296296297e-05,
      "loss": 0.1888,
      "step": 26600
    },
    {
      "epoch": 1.4805555555555556,
      "grad_norm": 0.6864321231842041,
      "learning_rate": 2.5325e-05,
      "loss": 0.1746,
      "step": 26650
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 2.621718168258667,
      "learning_rate": 2.5278703703703704e-05,
      "loss": 0.1782,
      "step": 26700
    },
    {
      "epoch": 1.4861111111111112,
      "grad_norm": 1.1373640298843384,
      "learning_rate": 2.523240740740741e-05,
      "loss": 0.2267,
      "step": 26750
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 1.1348049640655518,
      "learning_rate": 2.518611111111111e-05,
      "loss": 0.189,
      "step": 26800
    },
    {
      "epoch": 1.4916666666666667,
      "grad_norm": 0.6969485282897949,
      "learning_rate": 2.5139814814814817e-05,
      "loss": 0.1785,
      "step": 26850
    },
    {
      "epoch": 1.4944444444444445,
      "grad_norm": 0.8382731676101685,
      "learning_rate": 2.5093518518518522e-05,
      "loss": 0.2029,
      "step": 26900
    },
    {
      "epoch": 1.4972222222222222,
      "grad_norm": 0.27509644627571106,
      "learning_rate": 2.504722222222222e-05,
      "loss": 0.2053,
      "step": 26950
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.9581680297851562,
      "learning_rate": 2.5000925925925926e-05,
      "loss": 0.2319,
      "step": 27000
    },
    {
      "epoch": 1.5027777777777778,
      "grad_norm": 1.257317066192627,
      "learning_rate": 2.495462962962963e-05,
      "loss": 0.2292,
      "step": 27050
    },
    {
      "epoch": 1.5055555555555555,
      "grad_norm": 1.1261916160583496,
      "learning_rate": 2.4908333333333333e-05,
      "loss": 0.2231,
      "step": 27100
    },
    {
      "epoch": 1.5083333333333333,
      "grad_norm": 1.3071033954620361,
      "learning_rate": 2.486203703703704e-05,
      "loss": 0.1793,
      "step": 27150
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 2.588592290878296,
      "learning_rate": 2.481574074074074e-05,
      "loss": 0.1723,
      "step": 27200
    },
    {
      "epoch": 1.5138888888888888,
      "grad_norm": 0.9364189505577087,
      "learning_rate": 2.4769444444444446e-05,
      "loss": 0.1963,
      "step": 27250
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 1.0009169578552246,
      "learning_rate": 2.4723148148148148e-05,
      "loss": 0.2178,
      "step": 27300
    },
    {
      "epoch": 1.5194444444444444,
      "grad_norm": 0.945242166519165,
      "learning_rate": 2.4676851851851853e-05,
      "loss": 0.2182,
      "step": 27350
    },
    {
      "epoch": 1.5222222222222221,
      "grad_norm": 0.5802677273750305,
      "learning_rate": 2.4630555555555555e-05,
      "loss": 0.1874,
      "step": 27400
    },
    {
      "epoch": 1.525,
      "grad_norm": 1.280070185661316,
      "learning_rate": 2.458425925925926e-05,
      "loss": 0.2003,
      "step": 27450
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 0.49475598335266113,
      "learning_rate": 2.4537962962962966e-05,
      "loss": 0.2004,
      "step": 27500
    },
    {
      "epoch": 1.5305555555555554,
      "grad_norm": 1.275356650352478,
      "learning_rate": 2.4491666666666668e-05,
      "loss": 0.1939,
      "step": 27550
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.8239660859107971,
      "learning_rate": 2.444537037037037e-05,
      "loss": 0.1539,
      "step": 27600
    },
    {
      "epoch": 1.536111111111111,
      "grad_norm": 1.643413782119751,
      "learning_rate": 2.4399074074074075e-05,
      "loss": 0.2192,
      "step": 27650
    },
    {
      "epoch": 1.5388888888888888,
      "grad_norm": 0.7396786212921143,
      "learning_rate": 2.435277777777778e-05,
      "loss": 0.2258,
      "step": 27700
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 0.594764769077301,
      "learning_rate": 2.4306481481481483e-05,
      "loss": 0.1869,
      "step": 27750
    },
    {
      "epoch": 1.5444444444444443,
      "grad_norm": 1.9588096141815186,
      "learning_rate": 2.4260185185185184e-05,
      "loss": 0.2096,
      "step": 27800
    },
    {
      "epoch": 1.5472222222222223,
      "grad_norm": 1.050045371055603,
      "learning_rate": 2.421388888888889e-05,
      "loss": 0.2084,
      "step": 27850
    },
    {
      "epoch": 1.55,
      "grad_norm": 1.313154935836792,
      "learning_rate": 2.4167592592592595e-05,
      "loss": 0.1674,
      "step": 27900
    },
    {
      "epoch": 1.5527777777777778,
      "grad_norm": 1.6763660907745361,
      "learning_rate": 2.4121296296296297e-05,
      "loss": 0.2068,
      "step": 27950
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.7697842717170715,
      "learning_rate": 2.4075e-05,
      "loss": 0.1828,
      "step": 28000
    },
    {
      "epoch": 1.5583333333333333,
      "grad_norm": 1.571285605430603,
      "learning_rate": 2.4028703703703704e-05,
      "loss": 0.2125,
      "step": 28050
    },
    {
      "epoch": 1.5611111111111111,
      "grad_norm": 0.36459237337112427,
      "learning_rate": 2.398240740740741e-05,
      "loss": 0.1599,
      "step": 28100
    },
    {
      "epoch": 1.5638888888888889,
      "grad_norm": 0.7566168904304504,
      "learning_rate": 2.393611111111111e-05,
      "loss": 0.1911,
      "step": 28150
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.4041479825973511,
      "learning_rate": 2.3889814814814814e-05,
      "loss": 0.2289,
      "step": 28200
    },
    {
      "epoch": 1.5694444444444444,
      "grad_norm": 1.3500624895095825,
      "learning_rate": 2.384351851851852e-05,
      "loss": 0.1723,
      "step": 28250
    },
    {
      "epoch": 1.5722222222222222,
      "grad_norm": 0.6912826895713806,
      "learning_rate": 2.3797222222222224e-05,
      "loss": 0.1702,
      "step": 28300
    },
    {
      "epoch": 1.575,
      "grad_norm": 3.370047092437744,
      "learning_rate": 2.3750925925925926e-05,
      "loss": 0.2044,
      "step": 28350
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 1.084798812866211,
      "learning_rate": 2.3704629629629628e-05,
      "loss": 0.1652,
      "step": 28400
    },
    {
      "epoch": 1.5805555555555557,
      "grad_norm": 0.5946887731552124,
      "learning_rate": 2.3658333333333334e-05,
      "loss": 0.207,
      "step": 28450
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.7657567262649536,
      "learning_rate": 2.361203703703704e-05,
      "loss": 0.1907,
      "step": 28500
    },
    {
      "epoch": 1.5861111111111112,
      "grad_norm": 0.7842503190040588,
      "learning_rate": 2.356574074074074e-05,
      "loss": 0.1868,
      "step": 28550
    },
    {
      "epoch": 1.588888888888889,
      "grad_norm": 2.1007792949676514,
      "learning_rate": 2.3519444444444446e-05,
      "loss": 0.1974,
      "step": 28600
    },
    {
      "epoch": 1.5916666666666668,
      "grad_norm": 0.7580209970474243,
      "learning_rate": 2.3473148148148148e-05,
      "loss": 0.2184,
      "step": 28650
    },
    {
      "epoch": 1.5944444444444446,
      "grad_norm": 1.0161162614822388,
      "learning_rate": 2.3426851851851853e-05,
      "loss": 0.2171,
      "step": 28700
    },
    {
      "epoch": 1.5972222222222223,
      "grad_norm": 0.874319851398468,
      "learning_rate": 2.3380555555555555e-05,
      "loss": 0.1852,
      "step": 28750
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.5369327068328857,
      "learning_rate": 2.333425925925926e-05,
      "loss": 0.1644,
      "step": 28800
    },
    {
      "epoch": 1.6027777777777779,
      "grad_norm": 1.012623906135559,
      "learning_rate": 2.3287962962962963e-05,
      "loss": 0.1875,
      "step": 28850
    },
    {
      "epoch": 1.6055555555555556,
      "grad_norm": 2.2546660900115967,
      "learning_rate": 2.3241666666666668e-05,
      "loss": 0.197,
      "step": 28900
    },
    {
      "epoch": 1.6083333333333334,
      "grad_norm": 2.184863567352295,
      "learning_rate": 2.3195370370370373e-05,
      "loss": 0.201,
      "step": 28950
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 2.3728229999542236,
      "learning_rate": 2.3149074074074075e-05,
      "loss": 0.2024,
      "step": 29000
    },
    {
      "epoch": 1.613888888888889,
      "grad_norm": 2.901440382003784,
      "learning_rate": 2.3102777777777777e-05,
      "loss": 0.2118,
      "step": 29050
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 0.7444971799850464,
      "learning_rate": 2.3056481481481483e-05,
      "loss": 0.1612,
      "step": 29100
    },
    {
      "epoch": 1.6194444444444445,
      "grad_norm": 1.691558599472046,
      "learning_rate": 2.3010185185185188e-05,
      "loss": 0.188,
      "step": 29150
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 1.1057007312774658,
      "learning_rate": 2.296388888888889e-05,
      "loss": 0.1776,
      "step": 29200
    },
    {
      "epoch": 1.625,
      "grad_norm": 1.1010112762451172,
      "learning_rate": 2.2917592592592592e-05,
      "loss": 0.1692,
      "step": 29250
    },
    {
      "epoch": 1.6277777777777778,
      "grad_norm": 2.415210723876953,
      "learning_rate": 2.2871296296296297e-05,
      "loss": 0.2014,
      "step": 29300
    },
    {
      "epoch": 1.6305555555555555,
      "grad_norm": 1.768117904663086,
      "learning_rate": 2.2825000000000003e-05,
      "loss": 0.1791,
      "step": 29350
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 0.9683617949485779,
      "learning_rate": 2.2778703703703704e-05,
      "loss": 0.191,
      "step": 29400
    },
    {
      "epoch": 1.636111111111111,
      "grad_norm": 1.5943384170532227,
      "learning_rate": 2.2732407407407406e-05,
      "loss": 0.1953,
      "step": 29450
    },
    {
      "epoch": 1.6388888888888888,
      "grad_norm": 1.0662662982940674,
      "learning_rate": 2.2686111111111115e-05,
      "loss": 0.1476,
      "step": 29500
    },
    {
      "epoch": 1.6416666666666666,
      "grad_norm": 1.3699359893798828,
      "learning_rate": 2.2639814814814817e-05,
      "loss": 0.1681,
      "step": 29550
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 1.328414797782898,
      "learning_rate": 2.259351851851852e-05,
      "loss": 0.1887,
      "step": 29600
    },
    {
      "epoch": 1.6472222222222221,
      "grad_norm": 0.8184459805488586,
      "learning_rate": 2.254722222222222e-05,
      "loss": 0.1715,
      "step": 29650
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.7534785270690918,
      "learning_rate": 2.250092592592593e-05,
      "loss": 0.2017,
      "step": 29700
    },
    {
      "epoch": 1.6527777777777777,
      "grad_norm": 0.939439594745636,
      "learning_rate": 2.2454629629629632e-05,
      "loss": 0.197,
      "step": 29750
    },
    {
      "epoch": 1.6555555555555554,
      "grad_norm": 0.38143250346183777,
      "learning_rate": 2.2408333333333334e-05,
      "loss": 0.1742,
      "step": 29800
    },
    {
      "epoch": 1.6583333333333332,
      "grad_norm": 1.7370340824127197,
      "learning_rate": 2.2362037037037036e-05,
      "loss": 0.2284,
      "step": 29850
    },
    {
      "epoch": 1.661111111111111,
      "grad_norm": 1.8421341180801392,
      "learning_rate": 2.231574074074074e-05,
      "loss": 0.1745,
      "step": 29900
    },
    {
      "epoch": 1.6638888888888888,
      "grad_norm": 0.9967747926712036,
      "learning_rate": 2.2269444444444446e-05,
      "loss": 0.1676,
      "step": 29950
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.0034629106521606,
      "learning_rate": 2.2223148148148148e-05,
      "loss": 0.201,
      "step": 30000
    },
    {
      "epoch": 1.6694444444444443,
      "grad_norm": 1.8943086862564087,
      "learning_rate": 2.2176851851851854e-05,
      "loss": 0.1679,
      "step": 30050
    },
    {
      "epoch": 1.6722222222222223,
      "grad_norm": 0.809934139251709,
      "learning_rate": 2.2130555555555556e-05,
      "loss": 0.1773,
      "step": 30100
    },
    {
      "epoch": 1.675,
      "grad_norm": 1.4914066791534424,
      "learning_rate": 2.208425925925926e-05,
      "loss": 0.1682,
      "step": 30150
    },
    {
      "epoch": 1.6777777777777778,
      "grad_norm": 0.7150799632072449,
      "learning_rate": 2.2037962962962963e-05,
      "loss": 0.1684,
      "step": 30200
    },
    {
      "epoch": 1.6805555555555556,
      "grad_norm": 0.9061475396156311,
      "learning_rate": 2.1991666666666668e-05,
      "loss": 0.1495,
      "step": 30250
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 0.8357869386672974,
      "learning_rate": 2.194537037037037e-05,
      "loss": 0.1879,
      "step": 30300
    },
    {
      "epoch": 1.6861111111111111,
      "grad_norm": 0.6768651008605957,
      "learning_rate": 2.1899074074074075e-05,
      "loss": 0.1818,
      "step": 30350
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 1.4682629108428955,
      "learning_rate": 2.1852777777777777e-05,
      "loss": 0.1573,
      "step": 30400
    },
    {
      "epoch": 1.6916666666666667,
      "grad_norm": 0.9835178256034851,
      "learning_rate": 2.1806481481481483e-05,
      "loss": 0.1889,
      "step": 30450
    },
    {
      "epoch": 1.6944444444444444,
      "grad_norm": 0.9288529753684998,
      "learning_rate": 2.1760185185185185e-05,
      "loss": 0.2168,
      "step": 30500
    },
    {
      "epoch": 1.6972222222222222,
      "grad_norm": 3.429156541824341,
      "learning_rate": 2.171388888888889e-05,
      "loss": 0.1711,
      "step": 30550
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.4993120431900024,
      "learning_rate": 2.1667592592592595e-05,
      "loss": 0.2221,
      "step": 30600
    },
    {
      "epoch": 1.7027777777777777,
      "grad_norm": 0.8058891296386719,
      "learning_rate": 2.1621296296296297e-05,
      "loss": 0.2183,
      "step": 30650
    },
    {
      "epoch": 1.7055555555555557,
      "grad_norm": 1.0233116149902344,
      "learning_rate": 2.1575e-05,
      "loss": 0.1949,
      "step": 30700
    },
    {
      "epoch": 1.7083333333333335,
      "grad_norm": 0.8906661868095398,
      "learning_rate": 2.1528703703703705e-05,
      "loss": 0.1811,
      "step": 30750
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 7.716945171356201,
      "learning_rate": 2.148240740740741e-05,
      "loss": 0.1801,
      "step": 30800
    },
    {
      "epoch": 1.713888888888889,
      "grad_norm": 1.4530692100524902,
      "learning_rate": 2.1436111111111112e-05,
      "loss": 0.1816,
      "step": 30850
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 1.0419191122055054,
      "learning_rate": 2.1389814814814814e-05,
      "loss": 0.1991,
      "step": 30900
    },
    {
      "epoch": 1.7194444444444446,
      "grad_norm": 1.4853081703186035,
      "learning_rate": 2.134351851851852e-05,
      "loss": 0.1559,
      "step": 30950
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 1.4736601114273071,
      "learning_rate": 2.1297222222222225e-05,
      "loss": 0.1998,
      "step": 31000
    },
    {
      "epoch": 1.725,
      "grad_norm": 0.6426624655723572,
      "learning_rate": 2.1250925925925926e-05,
      "loss": 0.1716,
      "step": 31050
    },
    {
      "epoch": 1.7277777777777779,
      "grad_norm": 1.245350956916809,
      "learning_rate": 2.120462962962963e-05,
      "loss": 0.2137,
      "step": 31100
    },
    {
      "epoch": 1.7305555555555556,
      "grad_norm": 0.8343105316162109,
      "learning_rate": 2.1158333333333337e-05,
      "loss": 0.1636,
      "step": 31150
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 1.227440595626831,
      "learning_rate": 2.111203703703704e-05,
      "loss": 0.2007,
      "step": 31200
    },
    {
      "epoch": 1.7361111111111112,
      "grad_norm": 0.7307565808296204,
      "learning_rate": 2.106574074074074e-05,
      "loss": 0.2052,
      "step": 31250
    },
    {
      "epoch": 1.738888888888889,
      "grad_norm": 0.3550468981266022,
      "learning_rate": 2.1019444444444443e-05,
      "loss": 0.2187,
      "step": 31300
    },
    {
      "epoch": 1.7416666666666667,
      "grad_norm": 2.742724895477295,
      "learning_rate": 2.0973148148148152e-05,
      "loss": 0.16,
      "step": 31350
    },
    {
      "epoch": 1.7444444444444445,
      "grad_norm": 2.1644701957702637,
      "learning_rate": 2.0926851851851854e-05,
      "loss": 0.1889,
      "step": 31400
    },
    {
      "epoch": 1.7472222222222222,
      "grad_norm": 1.0140290260314941,
      "learning_rate": 2.0880555555555556e-05,
      "loss": 0.2284,
      "step": 31450
    },
    {
      "epoch": 1.75,
      "grad_norm": 1.7650952339172363,
      "learning_rate": 2.0834259259259258e-05,
      "loss": 0.2261,
      "step": 31500
    },
    {
      "epoch": 1.7527777777777778,
      "grad_norm": 0.6893970966339111,
      "learning_rate": 2.0787962962962966e-05,
      "loss": 0.1864,
      "step": 31550
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 0.7607628703117371,
      "learning_rate": 2.0741666666666668e-05,
      "loss": 0.2027,
      "step": 31600
    },
    {
      "epoch": 1.7583333333333333,
      "grad_norm": 0.5678070783615112,
      "learning_rate": 2.069537037037037e-05,
      "loss": 0.1952,
      "step": 31650
    },
    {
      "epoch": 1.761111111111111,
      "grad_norm": 4.748122215270996,
      "learning_rate": 2.0649074074074076e-05,
      "loss": 0.1855,
      "step": 31700
    },
    {
      "epoch": 1.7638888888888888,
      "grad_norm": 1.238445520401001,
      "learning_rate": 2.0602777777777778e-05,
      "loss": 0.1708,
      "step": 31750
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.7118746638298035,
      "learning_rate": 2.0556481481481483e-05,
      "loss": 0.1924,
      "step": 31800
    },
    {
      "epoch": 1.7694444444444444,
      "grad_norm": 4.001471519470215,
      "learning_rate": 2.0510185185185185e-05,
      "loss": 0.2159,
      "step": 31850
    },
    {
      "epoch": 1.7722222222222221,
      "grad_norm": 1.4477921724319458,
      "learning_rate": 2.046388888888889e-05,
      "loss": 0.1783,
      "step": 31900
    },
    {
      "epoch": 1.775,
      "grad_norm": 1.9978175163269043,
      "learning_rate": 2.0417592592592592e-05,
      "loss": 0.1804,
      "step": 31950
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 1.0364127159118652,
      "learning_rate": 2.0371296296296297e-05,
      "loss": 0.2145,
      "step": 32000
    },
    {
      "epoch": 1.7805555555555554,
      "grad_norm": 1.772348165512085,
      "learning_rate": 2.0325e-05,
      "loss": 0.1923,
      "step": 32050
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 0.9182054996490479,
      "learning_rate": 2.0278703703703705e-05,
      "loss": 0.1983,
      "step": 32100
    },
    {
      "epoch": 1.786111111111111,
      "grad_norm": 0.9012094736099243,
      "learning_rate": 2.0232407407407407e-05,
      "loss": 0.201,
      "step": 32150
    },
    {
      "epoch": 1.7888888888888888,
      "grad_norm": 1.9526594877243042,
      "learning_rate": 2.0186111111111112e-05,
      "loss": 0.208,
      "step": 32200
    },
    {
      "epoch": 1.7916666666666665,
      "grad_norm": 3.2581307888031006,
      "learning_rate": 2.0139814814814817e-05,
      "loss": 0.1664,
      "step": 32250
    },
    {
      "epoch": 1.7944444444444443,
      "grad_norm": 2.0348286628723145,
      "learning_rate": 2.009351851851852e-05,
      "loss": 0.1911,
      "step": 32300
    },
    {
      "epoch": 1.7972222222222223,
      "grad_norm": 0.8317106366157532,
      "learning_rate": 2.004722222222222e-05,
      "loss": 0.1885,
      "step": 32350
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.5034682750701904,
      "learning_rate": 2.0000925925925927e-05,
      "loss": 0.2366,
      "step": 32400
    },
    {
      "epoch": 1.8027777777777778,
      "grad_norm": 0.6186541318893433,
      "learning_rate": 1.9954629629629632e-05,
      "loss": 0.1716,
      "step": 32450
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 0.9504160284996033,
      "learning_rate": 1.9908333333333334e-05,
      "loss": 0.1713,
      "step": 32500
    },
    {
      "epoch": 1.8083333333333333,
      "grad_norm": 5.580896854400635,
      "learning_rate": 1.9862037037037036e-05,
      "loss": 0.2349,
      "step": 32550
    },
    {
      "epoch": 1.8111111111111111,
      "grad_norm": 0.9761031866073608,
      "learning_rate": 1.9815740740740745e-05,
      "loss": 0.1798,
      "step": 32600
    },
    {
      "epoch": 1.8138888888888889,
      "grad_norm": 2.549016237258911,
      "learning_rate": 1.9769444444444446e-05,
      "loss": 0.2026,
      "step": 32650
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 1.053374171257019,
      "learning_rate": 1.972314814814815e-05,
      "loss": 0.1838,
      "step": 32700
    },
    {
      "epoch": 1.8194444444444444,
      "grad_norm": 1.706198811531067,
      "learning_rate": 1.967685185185185e-05,
      "loss": 0.1914,
      "step": 32750
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 1.0502151250839233,
      "learning_rate": 1.963055555555556e-05,
      "loss": 0.2312,
      "step": 32800
    },
    {
      "epoch": 1.825,
      "grad_norm": 1.2342958450317383,
      "learning_rate": 1.958425925925926e-05,
      "loss": 0.17,
      "step": 32850
    },
    {
      "epoch": 1.8277777777777777,
      "grad_norm": 0.4525500535964966,
      "learning_rate": 1.9537962962962963e-05,
      "loss": 0.1785,
      "step": 32900
    },
    {
      "epoch": 1.8305555555555557,
      "grad_norm": 2.863663673400879,
      "learning_rate": 1.9491666666666665e-05,
      "loss": 0.1756,
      "step": 32950
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 2.192040205001831,
      "learning_rate": 1.9445370370370374e-05,
      "loss": 0.176,
      "step": 33000
    },
    {
      "epoch": 1.8361111111111112,
      "grad_norm": 1.14217209815979,
      "learning_rate": 1.9399074074074076e-05,
      "loss": 0.1766,
      "step": 33050
    },
    {
      "epoch": 1.838888888888889,
      "grad_norm": 0.9576775431632996,
      "learning_rate": 1.9352777777777778e-05,
      "loss": 0.185,
      "step": 33100
    },
    {
      "epoch": 1.8416666666666668,
      "grad_norm": 1.3024073839187622,
      "learning_rate": 1.9306481481481483e-05,
      "loss": 0.2117,
      "step": 33150
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 3.349792242050171,
      "learning_rate": 1.9260185185185188e-05,
      "loss": 0.1942,
      "step": 33200
    },
    {
      "epoch": 1.8472222222222223,
      "grad_norm": 1.4195863008499146,
      "learning_rate": 1.921388888888889e-05,
      "loss": 0.1733,
      "step": 33250
    },
    {
      "epoch": 1.85,
      "grad_norm": 1.6940163373947144,
      "learning_rate": 1.9167592592592592e-05,
      "loss": 0.1855,
      "step": 33300
    },
    {
      "epoch": 1.8527777777777779,
      "grad_norm": 1.3381576538085938,
      "learning_rate": 1.9121296296296298e-05,
      "loss": 0.1929,
      "step": 33350
    },
    {
      "epoch": 1.8555555555555556,
      "grad_norm": 1.2844411134719849,
      "learning_rate": 1.9075000000000003e-05,
      "loss": 0.1799,
      "step": 33400
    },
    {
      "epoch": 1.8583333333333334,
      "grad_norm": 0.37207627296447754,
      "learning_rate": 1.9028703703703705e-05,
      "loss": 0.2179,
      "step": 33450
    },
    {
      "epoch": 1.8611111111111112,
      "grad_norm": 2.538828134536743,
      "learning_rate": 1.8982407407407407e-05,
      "loss": 0.2045,
      "step": 33500
    },
    {
      "epoch": 1.863888888888889,
      "grad_norm": 1.4216989278793335,
      "learning_rate": 1.8936111111111112e-05,
      "loss": 0.1722,
      "step": 33550
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 1.1189672946929932,
      "learning_rate": 1.8889814814814814e-05,
      "loss": 0.1978,
      "step": 33600
    },
    {
      "epoch": 1.8694444444444445,
      "grad_norm": 1.8885242938995361,
      "learning_rate": 1.884351851851852e-05,
      "loss": 0.1856,
      "step": 33650
    },
    {
      "epoch": 1.8722222222222222,
      "grad_norm": 3.2453536987304688,
      "learning_rate": 1.8797222222222225e-05,
      "loss": 0.2273,
      "step": 33700
    },
    {
      "epoch": 1.875,
      "grad_norm": 0.9700157046318054,
      "learning_rate": 1.8750925925925927e-05,
      "loss": 0.1992,
      "step": 33750
    },
    {
      "epoch": 1.8777777777777778,
      "grad_norm": 0.6690748929977417,
      "learning_rate": 1.870462962962963e-05,
      "loss": 0.1866,
      "step": 33800
    },
    {
      "epoch": 1.8805555555555555,
      "grad_norm": 0.9282761216163635,
      "learning_rate": 1.8658333333333334e-05,
      "loss": 0.2138,
      "step": 33850
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 1.6286380290985107,
      "learning_rate": 1.861203703703704e-05,
      "loss": 0.1895,
      "step": 33900
    },
    {
      "epoch": 1.886111111111111,
      "grad_norm": 1.0747638940811157,
      "learning_rate": 1.856574074074074e-05,
      "loss": 0.2126,
      "step": 33950
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.5759756565093994,
      "learning_rate": 1.8519444444444443e-05,
      "loss": 0.2377,
      "step": 34000
    },
    {
      "epoch": 1.8916666666666666,
      "grad_norm": 0.9529255032539368,
      "learning_rate": 1.847314814814815e-05,
      "loss": 0.1787,
      "step": 34050
    },
    {
      "epoch": 1.8944444444444444,
      "grad_norm": 0.5863566994667053,
      "learning_rate": 1.8426851851851854e-05,
      "loss": 0.2173,
      "step": 34100
    },
    {
      "epoch": 1.8972222222222221,
      "grad_norm": 0.8454096913337708,
      "learning_rate": 1.8380555555555556e-05,
      "loss": 0.1768,
      "step": 34150
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.418876051902771,
      "learning_rate": 1.8334259259259258e-05,
      "loss": 0.1917,
      "step": 34200
    },
    {
      "epoch": 1.9027777777777777,
      "grad_norm": 1.6443010568618774,
      "learning_rate": 1.8287962962962967e-05,
      "loss": 0.1675,
      "step": 34250
    },
    {
      "epoch": 1.9055555555555554,
      "grad_norm": 1.7412166595458984,
      "learning_rate": 1.824166666666667e-05,
      "loss": 0.1878,
      "step": 34300
    },
    {
      "epoch": 1.9083333333333332,
      "grad_norm": 1.6473944187164307,
      "learning_rate": 1.819537037037037e-05,
      "loss": 0.1994,
      "step": 34350
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 1.1511913537979126,
      "learning_rate": 1.8149074074074072e-05,
      "loss": 0.205,
      "step": 34400
    },
    {
      "epoch": 1.9138888888888888,
      "grad_norm": 2.1882052421569824,
      "learning_rate": 1.810277777777778e-05,
      "loss": 0.1699,
      "step": 34450
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 4.955166339874268,
      "learning_rate": 1.8056481481481483e-05,
      "loss": 0.1615,
      "step": 34500
    },
    {
      "epoch": 1.9194444444444443,
      "grad_norm": 0.4690493941307068,
      "learning_rate": 1.8010185185185185e-05,
      "loss": 0.1653,
      "step": 34550
    },
    {
      "epoch": 1.9222222222222223,
      "grad_norm": 0.8565324544906616,
      "learning_rate": 1.7963888888888887e-05,
      "loss": 0.192,
      "step": 34600
    },
    {
      "epoch": 1.925,
      "grad_norm": 1.1334023475646973,
      "learning_rate": 1.7917592592592596e-05,
      "loss": 0.2032,
      "step": 34650
    },
    {
      "epoch": 1.9277777777777778,
      "grad_norm": 1.0184226036071777,
      "learning_rate": 1.7871296296296298e-05,
      "loss": 0.1669,
      "step": 34700
    },
    {
      "epoch": 1.9305555555555556,
      "grad_norm": 1.8387588262557983,
      "learning_rate": 1.7825e-05,
      "loss": 0.1928,
      "step": 34750
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 1.0751326084136963,
      "learning_rate": 1.7778703703703705e-05,
      "loss": 0.199,
      "step": 34800
    },
    {
      "epoch": 1.9361111111111111,
      "grad_norm": 1.7241848707199097,
      "learning_rate": 1.773240740740741e-05,
      "loss": 0.1638,
      "step": 34850
    },
    {
      "epoch": 1.9388888888888889,
      "grad_norm": 0.8455529808998108,
      "learning_rate": 1.7686111111111112e-05,
      "loss": 0.2047,
      "step": 34900
    },
    {
      "epoch": 1.9416666666666667,
      "grad_norm": 1.0956881046295166,
      "learning_rate": 1.7639814814814814e-05,
      "loss": 0.2214,
      "step": 34950
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 1.9545743465423584,
      "learning_rate": 1.759351851851852e-05,
      "loss": 0.1766,
      "step": 35000
    },
    {
      "epoch": 1.9472222222222222,
      "grad_norm": 1.5657250881195068,
      "learning_rate": 1.7547222222222225e-05,
      "loss": 0.1948,
      "step": 35050
    },
    {
      "epoch": 1.95,
      "grad_norm": 1.4701420068740845,
      "learning_rate": 1.7500925925925927e-05,
      "loss": 0.1738,
      "step": 35100
    },
    {
      "epoch": 1.9527777777777777,
      "grad_norm": 1.1805920600891113,
      "learning_rate": 1.745462962962963e-05,
      "loss": 0.2161,
      "step": 35150
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 1.1759555339813232,
      "learning_rate": 1.7408333333333334e-05,
      "loss": 0.2091,
      "step": 35200
    },
    {
      "epoch": 1.9583333333333335,
      "grad_norm": 0.46134036779403687,
      "learning_rate": 1.736203703703704e-05,
      "loss": 0.1793,
      "step": 35250
    },
    {
      "epoch": 1.9611111111111112,
      "grad_norm": 0.907804548740387,
      "learning_rate": 1.731574074074074e-05,
      "loss": 0.1642,
      "step": 35300
    },
    {
      "epoch": 1.963888888888889,
      "grad_norm": 1.6170471906661987,
      "learning_rate": 1.7269444444444447e-05,
      "loss": 0.1493,
      "step": 35350
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 4.873018264770508,
      "learning_rate": 1.722314814814815e-05,
      "loss": 0.2135,
      "step": 35400
    },
    {
      "epoch": 1.9694444444444446,
      "grad_norm": 2.6749770641326904,
      "learning_rate": 1.717685185185185e-05,
      "loss": 0.1736,
      "step": 35450
    },
    {
      "epoch": 1.9722222222222223,
      "grad_norm": 0.8434791564941406,
      "learning_rate": 1.7130555555555556e-05,
      "loss": 0.1797,
      "step": 35500
    },
    {
      "epoch": 1.975,
      "grad_norm": 3.0354254245758057,
      "learning_rate": 1.708425925925926e-05,
      "loss": 0.1911,
      "step": 35550
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 1.979425311088562,
      "learning_rate": 1.7037962962962963e-05,
      "loss": 0.1877,
      "step": 35600
    },
    {
      "epoch": 1.9805555555555556,
      "grad_norm": 0.8591035008430481,
      "learning_rate": 1.6991666666666665e-05,
      "loss": 0.2054,
      "step": 35650
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 0.9603418111801147,
      "learning_rate": 1.6945370370370374e-05,
      "loss": 0.1906,
      "step": 35700
    },
    {
      "epoch": 1.9861111111111112,
      "grad_norm": 1.2365642786026,
      "learning_rate": 1.6899074074074076e-05,
      "loss": 0.1636,
      "step": 35750
    },
    {
      "epoch": 1.988888888888889,
      "grad_norm": 1.2294275760650635,
      "learning_rate": 1.6852777777777778e-05,
      "loss": 0.2167,
      "step": 35800
    },
    {
      "epoch": 1.9916666666666667,
      "grad_norm": 1.1810969114303589,
      "learning_rate": 1.680648148148148e-05,
      "loss": 0.2112,
      "step": 35850
    },
    {
      "epoch": 1.9944444444444445,
      "grad_norm": 0.630692720413208,
      "learning_rate": 1.676018518518519e-05,
      "loss": 0.195,
      "step": 35900
    },
    {
      "epoch": 1.9972222222222222,
      "grad_norm": 0.9835917353630066,
      "learning_rate": 1.671388888888889e-05,
      "loss": 0.2253,
      "step": 35950
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.582172155380249,
      "learning_rate": 1.6667592592592592e-05,
      "loss": 0.1855,
      "step": 36000
    },
    {
      "epoch": 2.0027777777777778,
      "grad_norm": 0.6206719279289246,
      "learning_rate": 1.6621296296296294e-05,
      "loss": 0.1497,
      "step": 36050
    },
    {
      "epoch": 2.0055555555555555,
      "grad_norm": 0.9352210164070129,
      "learning_rate": 1.6575000000000003e-05,
      "loss": 0.1217,
      "step": 36100
    },
    {
      "epoch": 2.0083333333333333,
      "grad_norm": 0.6093355417251587,
      "learning_rate": 1.6528703703703705e-05,
      "loss": 0.1663,
      "step": 36150
    },
    {
      "epoch": 2.011111111111111,
      "grad_norm": 0.7793878316879272,
      "learning_rate": 1.6482407407407407e-05,
      "loss": 0.1479,
      "step": 36200
    },
    {
      "epoch": 2.013888888888889,
      "grad_norm": 1.074215292930603,
      "learning_rate": 1.6436111111111112e-05,
      "loss": 0.1566,
      "step": 36250
    },
    {
      "epoch": 2.0166666666666666,
      "grad_norm": 1.6717607975006104,
      "learning_rate": 1.6389814814814818e-05,
      "loss": 0.1711,
      "step": 36300
    },
    {
      "epoch": 2.0194444444444444,
      "grad_norm": 1.0343859195709229,
      "learning_rate": 1.634351851851852e-05,
      "loss": 0.1772,
      "step": 36350
    },
    {
      "epoch": 2.022222222222222,
      "grad_norm": 0.7432430982589722,
      "learning_rate": 1.629722222222222e-05,
      "loss": 0.1577,
      "step": 36400
    },
    {
      "epoch": 2.025,
      "grad_norm": 2.046515703201294,
      "learning_rate": 1.6250925925925927e-05,
      "loss": 0.1603,
      "step": 36450
    },
    {
      "epoch": 2.0277777777777777,
      "grad_norm": 0.6959938406944275,
      "learning_rate": 1.6204629629629632e-05,
      "loss": 0.1421,
      "step": 36500
    },
    {
      "epoch": 2.0305555555555554,
      "grad_norm": 1.59246826171875,
      "learning_rate": 1.6158333333333334e-05,
      "loss": 0.1385,
      "step": 36550
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.6518225073814392,
      "learning_rate": 1.6112037037037036e-05,
      "loss": 0.1565,
      "step": 36600
    },
    {
      "epoch": 2.036111111111111,
      "grad_norm": 1.3556396961212158,
      "learning_rate": 1.606574074074074e-05,
      "loss": 0.1754,
      "step": 36650
    },
    {
      "epoch": 2.0388888888888888,
      "grad_norm": 0.4631500244140625,
      "learning_rate": 1.6019444444444447e-05,
      "loss": 0.1635,
      "step": 36700
    },
    {
      "epoch": 2.0416666666666665,
      "grad_norm": 1.1583842039108276,
      "learning_rate": 1.597314814814815e-05,
      "loss": 0.159,
      "step": 36750
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 0.9986236095428467,
      "learning_rate": 1.5926851851851854e-05,
      "loss": 0.1448,
      "step": 36800
    },
    {
      "epoch": 2.047222222222222,
      "grad_norm": 1.2751836776733398,
      "learning_rate": 1.5880555555555556e-05,
      "loss": 0.1815,
      "step": 36850
    },
    {
      "epoch": 2.05,
      "grad_norm": 1.235582947731018,
      "learning_rate": 1.583425925925926e-05,
      "loss": 0.1616,
      "step": 36900
    },
    {
      "epoch": 2.0527777777777776,
      "grad_norm": 0.7864605188369751,
      "learning_rate": 1.5787962962962963e-05,
      "loss": 0.1498,
      "step": 36950
    },
    {
      "epoch": 2.0555555555555554,
      "grad_norm": 0.6826263666152954,
      "learning_rate": 1.574166666666667e-05,
      "loss": 0.1485,
      "step": 37000
    },
    {
      "epoch": 2.058333333333333,
      "grad_norm": 0.7212793231010437,
      "learning_rate": 1.569537037037037e-05,
      "loss": 0.148,
      "step": 37050
    },
    {
      "epoch": 2.061111111111111,
      "grad_norm": 1.2011743783950806,
      "learning_rate": 1.5649074074074076e-05,
      "loss": 0.1457,
      "step": 37100
    },
    {
      "epoch": 2.063888888888889,
      "grad_norm": 0.6599147319793701,
      "learning_rate": 1.5602777777777778e-05,
      "loss": 0.1641,
      "step": 37150
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.8819331526756287,
      "learning_rate": 1.5556481481481483e-05,
      "loss": 0.1727,
      "step": 37200
    },
    {
      "epoch": 2.0694444444444446,
      "grad_norm": 1.7223299741744995,
      "learning_rate": 1.5510185185185185e-05,
      "loss": 0.1701,
      "step": 37250
    },
    {
      "epoch": 2.0722222222222224,
      "grad_norm": 2.3066256046295166,
      "learning_rate": 1.5463888888888887e-05,
      "loss": 0.1849,
      "step": 37300
    },
    {
      "epoch": 2.075,
      "grad_norm": 0.44157811999320984,
      "learning_rate": 1.5417592592592596e-05,
      "loss": 0.1985,
      "step": 37350
    },
    {
      "epoch": 2.077777777777778,
      "grad_norm": 0.9732646942138672,
      "learning_rate": 1.5371296296296298e-05,
      "loss": 0.1607,
      "step": 37400
    },
    {
      "epoch": 2.0805555555555557,
      "grad_norm": 0.9754387140274048,
      "learning_rate": 1.5325e-05,
      "loss": 0.1719,
      "step": 37450
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 5.2505950927734375,
      "learning_rate": 1.5278703703703702e-05,
      "loss": 0.1632,
      "step": 37500
    },
    {
      "epoch": 2.0861111111111112,
      "grad_norm": 0.659315824508667,
      "learning_rate": 1.5232407407407409e-05,
      "loss": 0.1726,
      "step": 37550
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 0.7616708874702454,
      "learning_rate": 1.5186111111111112e-05,
      "loss": 0.1257,
      "step": 37600
    },
    {
      "epoch": 2.091666666666667,
      "grad_norm": 1.0414001941680908,
      "learning_rate": 1.5139814814814814e-05,
      "loss": 0.1705,
      "step": 37650
    },
    {
      "epoch": 2.0944444444444446,
      "grad_norm": 1.0007545948028564,
      "learning_rate": 1.5093518518518518e-05,
      "loss": 0.1906,
      "step": 37700
    },
    {
      "epoch": 2.0972222222222223,
      "grad_norm": 0.8280997276306152,
      "learning_rate": 1.5047222222222223e-05,
      "loss": 0.1504,
      "step": 37750
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.42542412877082825,
      "learning_rate": 1.5000925925925927e-05,
      "loss": 0.1566,
      "step": 37800
    },
    {
      "epoch": 2.102777777777778,
      "grad_norm": 1.0066778659820557,
      "learning_rate": 1.4954629629629629e-05,
      "loss": 0.1705,
      "step": 37850
    },
    {
      "epoch": 2.1055555555555556,
      "grad_norm": 0.4511720538139343,
      "learning_rate": 1.4908333333333336e-05,
      "loss": 0.1739,
      "step": 37900
    },
    {
      "epoch": 2.1083333333333334,
      "grad_norm": 0.9273155331611633,
      "learning_rate": 1.4862037037037038e-05,
      "loss": 0.1871,
      "step": 37950
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 1.6219571828842163,
      "learning_rate": 1.4815740740740742e-05,
      "loss": 0.1463,
      "step": 38000
    },
    {
      "epoch": 2.113888888888889,
      "grad_norm": 1.1638373136520386,
      "learning_rate": 1.4769444444444444e-05,
      "loss": 0.148,
      "step": 38050
    },
    {
      "epoch": 2.1166666666666667,
      "grad_norm": 1.0762735605239868,
      "learning_rate": 1.472314814814815e-05,
      "loss": 0.1536,
      "step": 38100
    },
    {
      "epoch": 2.1194444444444445,
      "grad_norm": 0.46811559796333313,
      "learning_rate": 1.4676851851851853e-05,
      "loss": 0.1463,
      "step": 38150
    },
    {
      "epoch": 2.1222222222222222,
      "grad_norm": 0.38988009095191956,
      "learning_rate": 1.4630555555555556e-05,
      "loss": 0.1526,
      "step": 38200
    },
    {
      "epoch": 2.125,
      "grad_norm": 2.2395424842834473,
      "learning_rate": 1.4584259259259258e-05,
      "loss": 0.1491,
      "step": 38250
    },
    {
      "epoch": 2.1277777777777778,
      "grad_norm": 1.117798924446106,
      "learning_rate": 1.4537962962962965e-05,
      "loss": 0.1425,
      "step": 38300
    },
    {
      "epoch": 2.1305555555555555,
      "grad_norm": 0.5368340611457825,
      "learning_rate": 1.4491666666666667e-05,
      "loss": 0.1595,
      "step": 38350
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.9362407922744751,
      "learning_rate": 1.444537037037037e-05,
      "loss": 0.1514,
      "step": 38400
    },
    {
      "epoch": 2.136111111111111,
      "grad_norm": 0.8101012706756592,
      "learning_rate": 1.4399074074074076e-05,
      "loss": 0.1602,
      "step": 38450
    },
    {
      "epoch": 2.138888888888889,
      "grad_norm": 0.9594044089317322,
      "learning_rate": 1.4352777777777778e-05,
      "loss": 0.1724,
      "step": 38500
    },
    {
      "epoch": 2.1416666666666666,
      "grad_norm": 3.544269323348999,
      "learning_rate": 1.4306481481481482e-05,
      "loss": 0.1625,
      "step": 38550
    },
    {
      "epoch": 2.1444444444444444,
      "grad_norm": 0.830193817615509,
      "learning_rate": 1.4260185185185185e-05,
      "loss": 0.1656,
      "step": 38600
    },
    {
      "epoch": 2.147222222222222,
      "grad_norm": 0.7586142420768738,
      "learning_rate": 1.421388888888889e-05,
      "loss": 0.187,
      "step": 38650
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.5427026152610779,
      "learning_rate": 1.4167592592592593e-05,
      "loss": 0.1622,
      "step": 38700
    },
    {
      "epoch": 2.1527777777777777,
      "grad_norm": 1.356760025024414,
      "learning_rate": 1.4121296296296296e-05,
      "loss": 0.1805,
      "step": 38750
    },
    {
      "epoch": 2.1555555555555554,
      "grad_norm": 2.2083237171173096,
      "learning_rate": 1.4075e-05,
      "loss": 0.1499,
      "step": 38800
    },
    {
      "epoch": 2.158333333333333,
      "grad_norm": 1.14111328125,
      "learning_rate": 1.4028703703703705e-05,
      "loss": 0.1479,
      "step": 38850
    },
    {
      "epoch": 2.161111111111111,
      "grad_norm": 1.8805822134017944,
      "learning_rate": 1.3982407407407407e-05,
      "loss": 0.176,
      "step": 38900
    },
    {
      "epoch": 2.1638888888888888,
      "grad_norm": 1.61317777633667,
      "learning_rate": 1.3936111111111111e-05,
      "loss": 0.1676,
      "step": 38950
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 2.462937593460083,
      "learning_rate": 1.3889814814814816e-05,
      "loss": 0.1976,
      "step": 39000
    },
    {
      "epoch": 2.1694444444444443,
      "grad_norm": 0.8907577395439148,
      "learning_rate": 1.384351851851852e-05,
      "loss": 0.1708,
      "step": 39050
    },
    {
      "epoch": 2.172222222222222,
      "grad_norm": 0.4079194962978363,
      "learning_rate": 1.3797222222222222e-05,
      "loss": 0.1522,
      "step": 39100
    },
    {
      "epoch": 2.175,
      "grad_norm": 0.5306629538536072,
      "learning_rate": 1.3750925925925925e-05,
      "loss": 0.1795,
      "step": 39150
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 1.6934148073196411,
      "learning_rate": 1.370462962962963e-05,
      "loss": 0.1553,
      "step": 39200
    },
    {
      "epoch": 2.1805555555555554,
      "grad_norm": 1.0843151807785034,
      "learning_rate": 1.3658333333333334e-05,
      "loss": 0.1922,
      "step": 39250
    },
    {
      "epoch": 2.183333333333333,
      "grad_norm": 3.8202598094940186,
      "learning_rate": 1.3612037037037036e-05,
      "loss": 0.1743,
      "step": 39300
    },
    {
      "epoch": 2.186111111111111,
      "grad_norm": 2.83901047706604,
      "learning_rate": 1.3565740740740743e-05,
      "loss": 0.168,
      "step": 39350
    },
    {
      "epoch": 2.188888888888889,
      "grad_norm": 1.3901692628860474,
      "learning_rate": 1.3519444444444445e-05,
      "loss": 0.1545,
      "step": 39400
    },
    {
      "epoch": 2.191666666666667,
      "grad_norm": 1.5169082880020142,
      "learning_rate": 1.3473148148148149e-05,
      "loss": 0.1436,
      "step": 39450
    },
    {
      "epoch": 2.1944444444444446,
      "grad_norm": 0.4469934403896332,
      "learning_rate": 1.3426851851851851e-05,
      "loss": 0.1586,
      "step": 39500
    },
    {
      "epoch": 2.1972222222222224,
      "grad_norm": 1.182298183441162,
      "learning_rate": 1.3380555555555558e-05,
      "loss": 0.1557,
      "step": 39550
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.9566529989242554,
      "learning_rate": 1.333425925925926e-05,
      "loss": 0.1566,
      "step": 39600
    },
    {
      "epoch": 2.202777777777778,
      "grad_norm": 2.0388667583465576,
      "learning_rate": 1.3287962962962964e-05,
      "loss": 0.1443,
      "step": 39650
    },
    {
      "epoch": 2.2055555555555557,
      "grad_norm": 1.1665747165679932,
      "learning_rate": 1.3241666666666666e-05,
      "loss": 0.1644,
      "step": 39700
    },
    {
      "epoch": 2.2083333333333335,
      "grad_norm": 1.3873528242111206,
      "learning_rate": 1.3195370370370373e-05,
      "loss": 0.1679,
      "step": 39750
    },
    {
      "epoch": 2.2111111111111112,
      "grad_norm": 1.0121665000915527,
      "learning_rate": 1.3149074074074075e-05,
      "loss": 0.1694,
      "step": 39800
    },
    {
      "epoch": 2.213888888888889,
      "grad_norm": 1.0522704124450684,
      "learning_rate": 1.3102777777777778e-05,
      "loss": 0.1497,
      "step": 39850
    },
    {
      "epoch": 2.216666666666667,
      "grad_norm": 1.2091035842895508,
      "learning_rate": 1.3056481481481483e-05,
      "loss": 0.1639,
      "step": 39900
    },
    {
      "epoch": 2.2194444444444446,
      "grad_norm": 1.4191974401474,
      "learning_rate": 1.3010185185185187e-05,
      "loss": 0.1672,
      "step": 39950
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.3046042919158936,
      "learning_rate": 1.2963888888888889e-05,
      "loss": 0.1587,
      "step": 40000
    },
    {
      "epoch": 2.225,
      "grad_norm": 0.6410378813743591,
      "learning_rate": 1.2917592592592593e-05,
      "loss": 0.1733,
      "step": 40050
    },
    {
      "epoch": 2.227777777777778,
      "grad_norm": 2.359255790710449,
      "learning_rate": 1.2871296296296298e-05,
      "loss": 0.191,
      "step": 40100
    },
    {
      "epoch": 2.2305555555555556,
      "grad_norm": 5.421903133392334,
      "learning_rate": 1.2825000000000002e-05,
      "loss": 0.1884,
      "step": 40150
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.0922364592552185,
      "learning_rate": 1.2778703703703704e-05,
      "loss": 0.1596,
      "step": 40200
    },
    {
      "epoch": 2.236111111111111,
      "grad_norm": 1.5112156867980957,
      "learning_rate": 1.2732407407407407e-05,
      "loss": 0.1896,
      "step": 40250
    },
    {
      "epoch": 2.238888888888889,
      "grad_norm": 1.043752670288086,
      "learning_rate": 1.2686111111111113e-05,
      "loss": 0.1918,
      "step": 40300
    },
    {
      "epoch": 2.2416666666666667,
      "grad_norm": 1.4362986087799072,
      "learning_rate": 1.2639814814814815e-05,
      "loss": 0.189,
      "step": 40350
    },
    {
      "epoch": 2.2444444444444445,
      "grad_norm": 1.4975570440292358,
      "learning_rate": 1.2593518518518518e-05,
      "loss": 0.18,
      "step": 40400
    },
    {
      "epoch": 2.2472222222222222,
      "grad_norm": 1.008758544921875,
      "learning_rate": 1.2547222222222224e-05,
      "loss": 0.162,
      "step": 40450
    },
    {
      "epoch": 2.25,
      "grad_norm": 1.3333450555801392,
      "learning_rate": 1.2500925925925927e-05,
      "loss": 0.1368,
      "step": 40500
    },
    {
      "epoch": 2.2527777777777778,
      "grad_norm": 1.2148710489273071,
      "learning_rate": 1.245462962962963e-05,
      "loss": 0.1923,
      "step": 40550
    },
    {
      "epoch": 2.2555555555555555,
      "grad_norm": 3.117238759994507,
      "learning_rate": 1.2408333333333335e-05,
      "loss": 0.1546,
      "step": 40600
    },
    {
      "epoch": 2.2583333333333333,
      "grad_norm": 0.908394992351532,
      "learning_rate": 1.2362037037037036e-05,
      "loss": 0.1677,
      "step": 40650
    },
    {
      "epoch": 2.261111111111111,
      "grad_norm": 0.4137120246887207,
      "learning_rate": 1.2315740740740742e-05,
      "loss": 0.153,
      "step": 40700
    },
    {
      "epoch": 2.263888888888889,
      "grad_norm": 2.1342344284057617,
      "learning_rate": 1.2269444444444444e-05,
      "loss": 0.1246,
      "step": 40750
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.2902681231498718,
      "learning_rate": 1.2223148148148149e-05,
      "loss": 0.1816,
      "step": 40800
    },
    {
      "epoch": 2.2694444444444444,
      "grad_norm": 1.1415373086929321,
      "learning_rate": 1.2176851851851851e-05,
      "loss": 0.1523,
      "step": 40850
    },
    {
      "epoch": 2.272222222222222,
      "grad_norm": 0.7139278054237366,
      "learning_rate": 1.2130555555555556e-05,
      "loss": 0.1747,
      "step": 40900
    },
    {
      "epoch": 2.275,
      "grad_norm": 1.2475863695144653,
      "learning_rate": 1.208425925925926e-05,
      "loss": 0.1491,
      "step": 40950
    },
    {
      "epoch": 2.2777777777777777,
      "grad_norm": 0.6305541396141052,
      "learning_rate": 1.2037962962962964e-05,
      "loss": 0.1664,
      "step": 41000
    },
    {
      "epoch": 2.2805555555555554,
      "grad_norm": 1.8885587453842163,
      "learning_rate": 1.1991666666666667e-05,
      "loss": 0.1657,
      "step": 41050
    },
    {
      "epoch": 2.283333333333333,
      "grad_norm": 1.2450693845748901,
      "learning_rate": 1.1945370370370371e-05,
      "loss": 0.1616,
      "step": 41100
    },
    {
      "epoch": 2.286111111111111,
      "grad_norm": 0.528438150882721,
      "learning_rate": 1.1899074074074075e-05,
      "loss": 0.1978,
      "step": 41150
    },
    {
      "epoch": 2.2888888888888888,
      "grad_norm": 1.077021837234497,
      "learning_rate": 1.1852777777777778e-05,
      "loss": 0.1953,
      "step": 41200
    },
    {
      "epoch": 2.2916666666666665,
      "grad_norm": 1.0226179361343384,
      "learning_rate": 1.1806481481481482e-05,
      "loss": 0.1742,
      "step": 41250
    },
    {
      "epoch": 2.2944444444444443,
      "grad_norm": 2.7295303344726562,
      "learning_rate": 1.1760185185185186e-05,
      "loss": 0.1557,
      "step": 41300
    },
    {
      "epoch": 2.297222222222222,
      "grad_norm": 0.7590492367744446,
      "learning_rate": 1.171388888888889e-05,
      "loss": 0.171,
      "step": 41350
    },
    {
      "epoch": 2.3,
      "grad_norm": 2.9884567260742188,
      "learning_rate": 1.1667592592592593e-05,
      "loss": 0.1679,
      "step": 41400
    },
    {
      "epoch": 2.3027777777777776,
      "grad_norm": 1.4610595703125,
      "learning_rate": 1.1621296296296296e-05,
      "loss": 0.1716,
      "step": 41450
    },
    {
      "epoch": 2.3055555555555554,
      "grad_norm": 0.6259527206420898,
      "learning_rate": 1.1575000000000002e-05,
      "loss": 0.1698,
      "step": 41500
    },
    {
      "epoch": 2.3083333333333336,
      "grad_norm": 2.0076491832733154,
      "learning_rate": 1.1528703703703704e-05,
      "loss": 0.1703,
      "step": 41550
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 0.9728208184242249,
      "learning_rate": 1.1482407407407409e-05,
      "loss": 0.1911,
      "step": 41600
    },
    {
      "epoch": 2.313888888888889,
      "grad_norm": 1.2107831239700317,
      "learning_rate": 1.1436111111111111e-05,
      "loss": 0.1504,
      "step": 41650
    },
    {
      "epoch": 2.3166666666666664,
      "grad_norm": 0.6430736184120178,
      "learning_rate": 1.1389814814814816e-05,
      "loss": 0.1992,
      "step": 41700
    },
    {
      "epoch": 2.3194444444444446,
      "grad_norm": 1.2720860242843628,
      "learning_rate": 1.1343518518518518e-05,
      "loss": 0.1943,
      "step": 41750
    },
    {
      "epoch": 2.3222222222222224,
      "grad_norm": 0.5844306945800781,
      "learning_rate": 1.1297222222222224e-05,
      "loss": 0.176,
      "step": 41800
    },
    {
      "epoch": 2.325,
      "grad_norm": 0.42691680788993835,
      "learning_rate": 1.1250925925925926e-05,
      "loss": 0.1683,
      "step": 41850
    },
    {
      "epoch": 2.327777777777778,
      "grad_norm": 0.6396538019180298,
      "learning_rate": 1.1204629629629631e-05,
      "loss": 0.1648,
      "step": 41900
    },
    {
      "epoch": 2.3305555555555557,
      "grad_norm": 1.0525627136230469,
      "learning_rate": 1.1158333333333335e-05,
      "loss": 0.1677,
      "step": 41950
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.6910330057144165,
      "learning_rate": 1.1112037037037038e-05,
      "loss": 0.174,
      "step": 42000
    },
    {
      "epoch": 2.3361111111111112,
      "grad_norm": 1.7637507915496826,
      "learning_rate": 1.1065740740740742e-05,
      "loss": 0.1642,
      "step": 42050
    },
    {
      "epoch": 2.338888888888889,
      "grad_norm": 0.5519499778747559,
      "learning_rate": 1.1019444444444446e-05,
      "loss": 0.1768,
      "step": 42100
    },
    {
      "epoch": 2.341666666666667,
      "grad_norm": 1.5183584690093994,
      "learning_rate": 1.097314814814815e-05,
      "loss": 0.1383,
      "step": 42150
    },
    {
      "epoch": 2.3444444444444446,
      "grad_norm": 0.969649612903595,
      "learning_rate": 1.0926851851851851e-05,
      "loss": 0.1823,
      "step": 42200
    },
    {
      "epoch": 2.3472222222222223,
      "grad_norm": 1.1563488245010376,
      "learning_rate": 1.0880555555555557e-05,
      "loss": 0.1377,
      "step": 42250
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.3546648323535919,
      "learning_rate": 1.0834259259259258e-05,
      "loss": 0.1616,
      "step": 42300
    },
    {
      "epoch": 2.352777777777778,
      "grad_norm": 0.4745578467845917,
      "learning_rate": 1.0787962962962964e-05,
      "loss": 0.1844,
      "step": 42350
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 0.6441689729690552,
      "learning_rate": 1.0741666666666666e-05,
      "loss": 0.1744,
      "step": 42400
    },
    {
      "epoch": 2.3583333333333334,
      "grad_norm": 1.1085418462753296,
      "learning_rate": 1.0695370370370371e-05,
      "loss": 0.1626,
      "step": 42450
    },
    {
      "epoch": 2.361111111111111,
      "grad_norm": 0.7136127948760986,
      "learning_rate": 1.0649074074074075e-05,
      "loss": 0.1642,
      "step": 42500
    },
    {
      "epoch": 2.363888888888889,
      "grad_norm": 1.7136516571044922,
      "learning_rate": 1.0602777777777778e-05,
      "loss": 0.1898,
      "step": 42550
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 0.2692428231239319,
      "learning_rate": 1.0556481481481482e-05,
      "loss": 0.1332,
      "step": 42600
    },
    {
      "epoch": 2.3694444444444445,
      "grad_norm": 2.3168106079101562,
      "learning_rate": 1.0510185185185186e-05,
      "loss": 0.1708,
      "step": 42650
    },
    {
      "epoch": 2.3722222222222222,
      "grad_norm": 1.4347295761108398,
      "learning_rate": 1.046388888888889e-05,
      "loss": 0.1569,
      "step": 42700
    },
    {
      "epoch": 2.375,
      "grad_norm": 0.6097280979156494,
      "learning_rate": 1.0417592592592593e-05,
      "loss": 0.1781,
      "step": 42750
    },
    {
      "epoch": 2.3777777777777778,
      "grad_norm": 0.8224449157714844,
      "learning_rate": 1.0371296296296297e-05,
      "loss": 0.1399,
      "step": 42800
    },
    {
      "epoch": 2.3805555555555555,
      "grad_norm": 1.3915202617645264,
      "learning_rate": 1.0325e-05,
      "loss": 0.1689,
      "step": 42850
    },
    {
      "epoch": 2.3833333333333333,
      "grad_norm": 0.9001368880271912,
      "learning_rate": 1.0278703703703704e-05,
      "loss": 0.1335,
      "step": 42900
    },
    {
      "epoch": 2.386111111111111,
      "grad_norm": 0.6175971031188965,
      "learning_rate": 1.0232407407407408e-05,
      "loss": 0.1891,
      "step": 42950
    },
    {
      "epoch": 2.388888888888889,
      "grad_norm": 0.7732275724411011,
      "learning_rate": 1.0186111111111111e-05,
      "loss": 0.1506,
      "step": 43000
    },
    {
      "epoch": 2.3916666666666666,
      "grad_norm": 0.5383524298667908,
      "learning_rate": 1.0139814814814817e-05,
      "loss": 0.1754,
      "step": 43050
    },
    {
      "epoch": 2.3944444444444444,
      "grad_norm": 0.6409444212913513,
      "learning_rate": 1.0093518518518518e-05,
      "loss": 0.147,
      "step": 43100
    },
    {
      "epoch": 2.397222222222222,
      "grad_norm": 0.5014159083366394,
      "learning_rate": 1.0047222222222224e-05,
      "loss": 0.1504,
      "step": 43150
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.8871656656265259,
      "learning_rate": 1.0000925925925926e-05,
      "loss": 0.1507,
      "step": 43200
    },
    {
      "epoch": 2.4027777777777777,
      "grad_norm": 0.5004672408103943,
      "learning_rate": 9.954629629629631e-06,
      "loss": 0.1522,
      "step": 43250
    },
    {
      "epoch": 2.4055555555555554,
      "grad_norm": 0.9717675447463989,
      "learning_rate": 9.908333333333333e-06,
      "loss": 0.2187,
      "step": 43300
    },
    {
      "epoch": 2.408333333333333,
      "grad_norm": 1.1187500953674316,
      "learning_rate": 9.862037037037038e-06,
      "loss": 0.1667,
      "step": 43350
    },
    {
      "epoch": 2.411111111111111,
      "grad_norm": 1.5333846807479858,
      "learning_rate": 9.81574074074074e-06,
      "loss": 0.1777,
      "step": 43400
    },
    {
      "epoch": 2.4138888888888888,
      "grad_norm": 0.7637437582015991,
      "learning_rate": 9.769444444444446e-06,
      "loss": 0.1641,
      "step": 43450
    },
    {
      "epoch": 2.4166666666666665,
      "grad_norm": 0.5775116682052612,
      "learning_rate": 9.72314814814815e-06,
      "loss": 0.1507,
      "step": 43500
    },
    {
      "epoch": 2.4194444444444443,
      "grad_norm": 0.7983005046844482,
      "learning_rate": 9.676851851851853e-06,
      "loss": 0.1527,
      "step": 43550
    },
    {
      "epoch": 2.422222222222222,
      "grad_norm": 3.7259328365325928,
      "learning_rate": 9.630555555555557e-06,
      "loss": 0.1874,
      "step": 43600
    },
    {
      "epoch": 2.425,
      "grad_norm": 1.433367371559143,
      "learning_rate": 9.58425925925926e-06,
      "loss": 0.1709,
      "step": 43650
    },
    {
      "epoch": 2.4277777777777776,
      "grad_norm": 1.4369803667068481,
      "learning_rate": 9.537962962962964e-06,
      "loss": 0.1683,
      "step": 43700
    },
    {
      "epoch": 2.4305555555555554,
      "grad_norm": 1.187468409538269,
      "learning_rate": 9.491666666666668e-06,
      "loss": 0.1695,
      "step": 43750
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.7376317977905273,
      "learning_rate": 9.445370370370371e-06,
      "loss": 0.1495,
      "step": 43800
    },
    {
      "epoch": 2.436111111111111,
      "grad_norm": 3.098284959793091,
      "learning_rate": 9.399074074074075e-06,
      "loss": 0.122,
      "step": 43850
    },
    {
      "epoch": 2.438888888888889,
      "grad_norm": 1.8503167629241943,
      "learning_rate": 9.352777777777778e-06,
      "loss": 0.1751,
      "step": 43900
    },
    {
      "epoch": 2.4416666666666664,
      "grad_norm": 1.0809714794158936,
      "learning_rate": 9.306481481481482e-06,
      "loss": 0.1738,
      "step": 43950
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 1.3323428630828857,
      "learning_rate": 9.260185185185186e-06,
      "loss": 0.1582,
      "step": 44000
    },
    {
      "epoch": 2.4472222222222224,
      "grad_norm": 7.735678672790527,
      "learning_rate": 9.21388888888889e-06,
      "loss": 0.185,
      "step": 44050
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.738412082195282,
      "learning_rate": 9.167592592592593e-06,
      "loss": 0.1579,
      "step": 44100
    },
    {
      "epoch": 2.452777777777778,
      "grad_norm": 1.3757191896438599,
      "learning_rate": 9.121296296296297e-06,
      "loss": 0.1624,
      "step": 44150
    },
    {
      "epoch": 2.4555555555555557,
      "grad_norm": 1.5723636150360107,
      "learning_rate": 9.075e-06,
      "loss": 0.1817,
      "step": 44200
    },
    {
      "epoch": 2.4583333333333335,
      "grad_norm": 0.6074625253677368,
      "learning_rate": 9.028703703703704e-06,
      "loss": 0.1489,
      "step": 44250
    },
    {
      "epoch": 2.4611111111111112,
      "grad_norm": 2.377943992614746,
      "learning_rate": 8.982407407407408e-06,
      "loss": 0.1852,
      "step": 44300
    },
    {
      "epoch": 2.463888888888889,
      "grad_norm": 1.2299327850341797,
      "learning_rate": 8.936111111111111e-06,
      "loss": 0.1593,
      "step": 44350
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.881741464138031,
      "learning_rate": 8.889814814814815e-06,
      "loss": 0.1502,
      "step": 44400
    },
    {
      "epoch": 2.4694444444444446,
      "grad_norm": 13.739543914794922,
      "learning_rate": 8.843518518518519e-06,
      "loss": 0.185,
      "step": 44450
    },
    {
      "epoch": 2.4722222222222223,
      "grad_norm": 1.438951849937439,
      "learning_rate": 8.797222222222222e-06,
      "loss": 0.1696,
      "step": 44500
    },
    {
      "epoch": 2.475,
      "grad_norm": 0.5465516448020935,
      "learning_rate": 8.750925925925926e-06,
      "loss": 0.1731,
      "step": 44550
    },
    {
      "epoch": 2.477777777777778,
      "grad_norm": 1.2346769571304321,
      "learning_rate": 8.704629629629631e-06,
      "loss": 0.1755,
      "step": 44600
    },
    {
      "epoch": 2.4805555555555556,
      "grad_norm": 1.142132043838501,
      "learning_rate": 8.658333333333333e-06,
      "loss": 0.1926,
      "step": 44650
    },
    {
      "epoch": 2.4833333333333334,
      "grad_norm": 1.5488471984863281,
      "learning_rate": 8.612037037037039e-06,
      "loss": 0.1734,
      "step": 44700
    },
    {
      "epoch": 2.486111111111111,
      "grad_norm": 0.8818653225898743,
      "learning_rate": 8.56574074074074e-06,
      "loss": 0.1868,
      "step": 44750
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 0.8812942504882812,
      "learning_rate": 8.519444444444446e-06,
      "loss": 0.1583,
      "step": 44800
    },
    {
      "epoch": 2.4916666666666667,
      "grad_norm": 2.6611623764038086,
      "learning_rate": 8.473148148148148e-06,
      "loss": 0.1635,
      "step": 44850
    },
    {
      "epoch": 2.4944444444444445,
      "grad_norm": 1.7459481954574585,
      "learning_rate": 8.426851851851853e-06,
      "loss": 0.1733,
      "step": 44900
    },
    {
      "epoch": 2.4972222222222222,
      "grad_norm": 0.8832163214683533,
      "learning_rate": 8.380555555555555e-06,
      "loss": 0.1906,
      "step": 44950
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.2452077865600586,
      "learning_rate": 8.33425925925926e-06,
      "loss": 0.1705,
      "step": 45000
    },
    {
      "epoch": 2.5027777777777778,
      "grad_norm": 1.1893720626831055,
      "learning_rate": 8.287962962962964e-06,
      "loss": 0.159,
      "step": 45050
    },
    {
      "epoch": 2.5055555555555555,
      "grad_norm": 0.507364809513092,
      "learning_rate": 8.241666666666668e-06,
      "loss": 0.1662,
      "step": 45100
    },
    {
      "epoch": 2.5083333333333333,
      "grad_norm": 1.3944793939590454,
      "learning_rate": 8.195370370370371e-06,
      "loss": 0.1911,
      "step": 45150
    },
    {
      "epoch": 2.511111111111111,
      "grad_norm": 0.4341740012168884,
      "learning_rate": 8.149074074074075e-06,
      "loss": 0.1585,
      "step": 45200
    },
    {
      "epoch": 2.513888888888889,
      "grad_norm": 0.8810408115386963,
      "learning_rate": 8.102777777777779e-06,
      "loss": 0.1431,
      "step": 45250
    },
    {
      "epoch": 2.5166666666666666,
      "grad_norm": 1.0840109586715698,
      "learning_rate": 8.056481481481482e-06,
      "loss": 0.152,
      "step": 45300
    },
    {
      "epoch": 2.5194444444444444,
      "grad_norm": 0.8285109400749207,
      "learning_rate": 8.010185185185186e-06,
      "loss": 0.1667,
      "step": 45350
    },
    {
      "epoch": 2.522222222222222,
      "grad_norm": 0.8272605538368225,
      "learning_rate": 7.96388888888889e-06,
      "loss": 0.1465,
      "step": 45400
    },
    {
      "epoch": 2.525,
      "grad_norm": 0.8903933167457581,
      "learning_rate": 7.917592592592593e-06,
      "loss": 0.1474,
      "step": 45450
    },
    {
      "epoch": 2.5277777777777777,
      "grad_norm": 0.9215490818023682,
      "learning_rate": 7.871296296296297e-06,
      "loss": 0.1703,
      "step": 45500
    },
    {
      "epoch": 2.5305555555555554,
      "grad_norm": 1.7929424047470093,
      "learning_rate": 7.825e-06,
      "loss": 0.183,
      "step": 45550
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 4.5251359939575195,
      "learning_rate": 7.778703703703704e-06,
      "loss": 0.1534,
      "step": 45600
    },
    {
      "epoch": 2.536111111111111,
      "grad_norm": 0.5256050229072571,
      "learning_rate": 7.732407407407408e-06,
      "loss": 0.1506,
      "step": 45650
    },
    {
      "epoch": 2.5388888888888888,
      "grad_norm": 1.0010138750076294,
      "learning_rate": 7.686111111111111e-06,
      "loss": 0.17,
      "step": 45700
    },
    {
      "epoch": 2.5416666666666665,
      "grad_norm": 0.8342388272285461,
      "learning_rate": 7.639814814814815e-06,
      "loss": 0.1569,
      "step": 45750
    },
    {
      "epoch": 2.5444444444444443,
      "grad_norm": 0.9919812679290771,
      "learning_rate": 7.5935185185185195e-06,
      "loss": 0.1822,
      "step": 45800
    },
    {
      "epoch": 2.5472222222222225,
      "grad_norm": 2.00651216506958,
      "learning_rate": 7.547222222222222e-06,
      "loss": 0.1587,
      "step": 45850
    },
    {
      "epoch": 2.55,
      "grad_norm": 1.375627875328064,
      "learning_rate": 7.500925925925927e-06,
      "loss": 0.154,
      "step": 45900
    },
    {
      "epoch": 2.552777777777778,
      "grad_norm": 2.168231964111328,
      "learning_rate": 7.45462962962963e-06,
      "loss": 0.156,
      "step": 45950
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.7709840536117554,
      "learning_rate": 7.408333333333334e-06,
      "loss": 0.1727,
      "step": 46000
    },
    {
      "epoch": 2.5583333333333336,
      "grad_norm": 1.4020016193389893,
      "learning_rate": 7.362037037037037e-06,
      "loss": 0.1542,
      "step": 46050
    },
    {
      "epoch": 2.561111111111111,
      "grad_norm": 1.1686136722564697,
      "learning_rate": 7.315740740740741e-06,
      "loss": 0.1768,
      "step": 46100
    },
    {
      "epoch": 2.563888888888889,
      "grad_norm": 0.14345581829547882,
      "learning_rate": 7.269444444444445e-06,
      "loss": 0.1536,
      "step": 46150
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 1.436660885810852,
      "learning_rate": 7.223148148148149e-06,
      "loss": 0.1537,
      "step": 46200
    },
    {
      "epoch": 2.5694444444444446,
      "grad_norm": 0.873993992805481,
      "learning_rate": 7.176851851851852e-06,
      "loss": 0.1649,
      "step": 46250
    },
    {
      "epoch": 2.572222222222222,
      "grad_norm": 1.381191372871399,
      "learning_rate": 7.130555555555556e-06,
      "loss": 0.1553,
      "step": 46300
    },
    {
      "epoch": 2.575,
      "grad_norm": 0.49734440445899963,
      "learning_rate": 7.08425925925926e-06,
      "loss": 0.1891,
      "step": 46350
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 1.092842936515808,
      "learning_rate": 7.037962962962963e-06,
      "loss": 0.1524,
      "step": 46400
    },
    {
      "epoch": 2.5805555555555557,
      "grad_norm": 1.1374236345291138,
      "learning_rate": 6.991666666666667e-06,
      "loss": 0.1608,
      "step": 46450
    },
    {
      "epoch": 2.5833333333333335,
      "grad_norm": 1.2882219552993774,
      "learning_rate": 6.94537037037037e-06,
      "loss": 0.1902,
      "step": 46500
    },
    {
      "epoch": 2.5861111111111112,
      "grad_norm": 1.0464106798171997,
      "learning_rate": 6.899074074074074e-06,
      "loss": 0.1681,
      "step": 46550
    },
    {
      "epoch": 2.588888888888889,
      "grad_norm": 0.25626376271247864,
      "learning_rate": 6.852777777777779e-06,
      "loss": 0.147,
      "step": 46600
    },
    {
      "epoch": 2.591666666666667,
      "grad_norm": 0.8200761675834656,
      "learning_rate": 6.8064814814814815e-06,
      "loss": 0.1647,
      "step": 46650
    },
    {
      "epoch": 2.5944444444444446,
      "grad_norm": 1.492742657661438,
      "learning_rate": 6.760185185185186e-06,
      "loss": 0.1421,
      "step": 46700
    },
    {
      "epoch": 2.5972222222222223,
      "grad_norm": 1.0654464960098267,
      "learning_rate": 6.713888888888889e-06,
      "loss": 0.1441,
      "step": 46750
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.7285842299461365,
      "learning_rate": 6.667592592592593e-06,
      "loss": 0.1458,
      "step": 46800
    },
    {
      "epoch": 2.602777777777778,
      "grad_norm": 1.656681776046753,
      "learning_rate": 6.621296296296296e-06,
      "loss": 0.1616,
      "step": 46850
    },
    {
      "epoch": 2.6055555555555556,
      "grad_norm": 1.096049427986145,
      "learning_rate": 6.5750000000000006e-06,
      "loss": 0.162,
      "step": 46900
    },
    {
      "epoch": 2.6083333333333334,
      "grad_norm": 0.9285813570022583,
      "learning_rate": 6.528703703703703e-06,
      "loss": 0.1417,
      "step": 46950
    },
    {
      "epoch": 2.611111111111111,
      "grad_norm": 0.3725159466266632,
      "learning_rate": 6.482407407407408e-06,
      "loss": 0.1576,
      "step": 47000
    },
    {
      "epoch": 2.613888888888889,
      "grad_norm": 0.5152425169944763,
      "learning_rate": 6.436111111111111e-06,
      "loss": 0.1453,
      "step": 47050
    },
    {
      "epoch": 2.6166666666666667,
      "grad_norm": 0.8121613264083862,
      "learning_rate": 6.389814814814815e-06,
      "loss": 0.1721,
      "step": 47100
    },
    {
      "epoch": 2.6194444444444445,
      "grad_norm": 1.1948753595352173,
      "learning_rate": 6.34351851851852e-06,
      "loss": 0.1755,
      "step": 47150
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 0.5068606734275818,
      "learning_rate": 6.2972222222222224e-06,
      "loss": 0.1745,
      "step": 47200
    },
    {
      "epoch": 2.625,
      "grad_norm": 2.09108829498291,
      "learning_rate": 6.250925925925927e-06,
      "loss": 0.1595,
      "step": 47250
    },
    {
      "epoch": 2.6277777777777778,
      "grad_norm": 1.3371641635894775,
      "learning_rate": 6.20462962962963e-06,
      "loss": 0.1733,
      "step": 47300
    },
    {
      "epoch": 2.6305555555555555,
      "grad_norm": 1.1458951234817505,
      "learning_rate": 6.158333333333333e-06,
      "loss": 0.1573,
      "step": 47350
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 1.8353357315063477,
      "learning_rate": 6.112037037037038e-06,
      "loss": 0.1557,
      "step": 47400
    },
    {
      "epoch": 2.636111111111111,
      "grad_norm": 1.592851996421814,
      "learning_rate": 6.0657407407407415e-06,
      "loss": 0.1978,
      "step": 47450
    },
    {
      "epoch": 2.638888888888889,
      "grad_norm": 1.5571963787078857,
      "learning_rate": 6.019444444444445e-06,
      "loss": 0.1462,
      "step": 47500
    },
    {
      "epoch": 2.6416666666666666,
      "grad_norm": 0.9005954265594482,
      "learning_rate": 5.973148148148149e-06,
      "loss": 0.1834,
      "step": 47550
    },
    {
      "epoch": 2.6444444444444444,
      "grad_norm": 1.0369865894317627,
      "learning_rate": 5.9268518518518525e-06,
      "loss": 0.1743,
      "step": 47600
    },
    {
      "epoch": 2.647222222222222,
      "grad_norm": 0.9976364970207214,
      "learning_rate": 5.880555555555556e-06,
      "loss": 0.1678,
      "step": 47650
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.7934902906417847,
      "learning_rate": 5.83425925925926e-06,
      "loss": 0.1411,
      "step": 47700
    },
    {
      "epoch": 2.6527777777777777,
      "grad_norm": 1.7864397764205933,
      "learning_rate": 5.787962962962963e-06,
      "loss": 0.1728,
      "step": 47750
    },
    {
      "epoch": 2.6555555555555554,
      "grad_norm": 1.8551104068756104,
      "learning_rate": 5.741666666666667e-06,
      "loss": 0.1879,
      "step": 47800
    },
    {
      "epoch": 2.658333333333333,
      "grad_norm": 2.2779972553253174,
      "learning_rate": 5.695370370370371e-06,
      "loss": 0.1861,
      "step": 47850
    },
    {
      "epoch": 2.661111111111111,
      "grad_norm": 0.7004196643829346,
      "learning_rate": 5.649074074074074e-06,
      "loss": 0.1541,
      "step": 47900
    },
    {
      "epoch": 2.6638888888888888,
      "grad_norm": 3.1119937896728516,
      "learning_rate": 5.602777777777778e-06,
      "loss": 0.1686,
      "step": 47950
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 1.5005202293395996,
      "learning_rate": 5.556481481481482e-06,
      "loss": 0.1456,
      "step": 48000
    },
    {
      "epoch": 2.6694444444444443,
      "grad_norm": 1.1636966466903687,
      "learning_rate": 5.510185185185185e-06,
      "loss": 0.1495,
      "step": 48050
    },
    {
      "epoch": 2.6722222222222225,
      "grad_norm": 0.7347843647003174,
      "learning_rate": 5.463888888888889e-06,
      "loss": 0.1478,
      "step": 48100
    },
    {
      "epoch": 2.675,
      "grad_norm": 0.3950919210910797,
      "learning_rate": 5.4175925925925925e-06,
      "loss": 0.1379,
      "step": 48150
    },
    {
      "epoch": 2.677777777777778,
      "grad_norm": 1.2395347356796265,
      "learning_rate": 5.371296296296296e-06,
      "loss": 0.1569,
      "step": 48200
    },
    {
      "epoch": 2.6805555555555554,
      "grad_norm": 1.7536494731903076,
      "learning_rate": 5.325e-06,
      "loss": 0.1544,
      "step": 48250
    },
    {
      "epoch": 2.6833333333333336,
      "grad_norm": 0.6402456760406494,
      "learning_rate": 5.2787037037037035e-06,
      "loss": 0.1878,
      "step": 48300
    },
    {
      "epoch": 2.686111111111111,
      "grad_norm": 2.188598394393921,
      "learning_rate": 5.232407407407407e-06,
      "loss": 0.1559,
      "step": 48350
    },
    {
      "epoch": 2.688888888888889,
      "grad_norm": 1.636898159980774,
      "learning_rate": 5.186111111111111e-06,
      "loss": 0.1789,
      "step": 48400
    },
    {
      "epoch": 2.6916666666666664,
      "grad_norm": 1.8433607816696167,
      "learning_rate": 5.139814814814815e-06,
      "loss": 0.1468,
      "step": 48450
    },
    {
      "epoch": 2.6944444444444446,
      "grad_norm": 2.5779640674591064,
      "learning_rate": 5.093518518518519e-06,
      "loss": 0.1544,
      "step": 48500
    },
    {
      "epoch": 2.697222222222222,
      "grad_norm": 4.022229194641113,
      "learning_rate": 5.0472222222222226e-06,
      "loss": 0.1708,
      "step": 48550
    },
    {
      "epoch": 2.7,
      "grad_norm": 1.9670928716659546,
      "learning_rate": 5.000925925925926e-06,
      "loss": 0.1696,
      "step": 48600
    },
    {
      "epoch": 2.7027777777777775,
      "grad_norm": 2.152820587158203,
      "learning_rate": 4.95462962962963e-06,
      "loss": 0.1508,
      "step": 48650
    },
    {
      "epoch": 2.7055555555555557,
      "grad_norm": 0.6885342001914978,
      "learning_rate": 4.9083333333333335e-06,
      "loss": 0.1918,
      "step": 48700
    },
    {
      "epoch": 2.7083333333333335,
      "grad_norm": 1.4493896961212158,
      "learning_rate": 4.862037037037037e-06,
      "loss": 0.1775,
      "step": 48750
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 1.166230320930481,
      "learning_rate": 4.815740740740741e-06,
      "loss": 0.175,
      "step": 48800
    },
    {
      "epoch": 2.713888888888889,
      "grad_norm": 0.530790388584137,
      "learning_rate": 4.7694444444444444e-06,
      "loss": 0.1799,
      "step": 48850
    },
    {
      "epoch": 2.716666666666667,
      "grad_norm": 1.201701045036316,
      "learning_rate": 4.723148148148148e-06,
      "loss": 0.1818,
      "step": 48900
    },
    {
      "epoch": 2.7194444444444446,
      "grad_norm": 1.287778615951538,
      "learning_rate": 4.6768518518518526e-06,
      "loss": 0.1708,
      "step": 48950
    },
    {
      "epoch": 2.7222222222222223,
      "grad_norm": 0.21856294572353363,
      "learning_rate": 4.630555555555556e-06,
      "loss": 0.173,
      "step": 49000
    },
    {
      "epoch": 2.725,
      "grad_norm": 0.35429540276527405,
      "learning_rate": 4.58425925925926e-06,
      "loss": 0.173,
      "step": 49050
    },
    {
      "epoch": 2.727777777777778,
      "grad_norm": 0.746450662612915,
      "learning_rate": 4.5379629629629635e-06,
      "loss": 0.1377,
      "step": 49100
    },
    {
      "epoch": 2.7305555555555556,
      "grad_norm": 1.1434484720230103,
      "learning_rate": 4.491666666666667e-06,
      "loss": 0.1477,
      "step": 49150
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 2.2692081928253174,
      "learning_rate": 4.445370370370371e-06,
      "loss": 0.1758,
      "step": 49200
    },
    {
      "epoch": 2.736111111111111,
      "grad_norm": 1.0747171640396118,
      "learning_rate": 4.3990740740740744e-06,
      "loss": 0.183,
      "step": 49250
    },
    {
      "epoch": 2.738888888888889,
      "grad_norm": 0.9681972861289978,
      "learning_rate": 4.352777777777778e-06,
      "loss": 0.149,
      "step": 49300
    },
    {
      "epoch": 2.7416666666666667,
      "grad_norm": 1.582427978515625,
      "learning_rate": 4.306481481481482e-06,
      "loss": 0.1583,
      "step": 49350
    },
    {
      "epoch": 2.7444444444444445,
      "grad_norm": 1.4997243881225586,
      "learning_rate": 4.260185185185185e-06,
      "loss": 0.1819,
      "step": 49400
    },
    {
      "epoch": 2.7472222222222222,
      "grad_norm": 1.6976115703582764,
      "learning_rate": 4.213888888888889e-06,
      "loss": 0.1584,
      "step": 49450
    },
    {
      "epoch": 2.75,
      "grad_norm": 1.346409559249878,
      "learning_rate": 4.167592592592593e-06,
      "loss": 0.1635,
      "step": 49500
    },
    {
      "epoch": 2.7527777777777778,
      "grad_norm": 2.0488624572753906,
      "learning_rate": 4.121296296296296e-06,
      "loss": 0.1635,
      "step": 49550
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 1.0108847618103027,
      "learning_rate": 4.075e-06,
      "loss": 0.1586,
      "step": 49600
    },
    {
      "epoch": 2.7583333333333333,
      "grad_norm": 1.3274264335632324,
      "learning_rate": 4.028703703703704e-06,
      "loss": 0.1742,
      "step": 49650
    },
    {
      "epoch": 2.761111111111111,
      "grad_norm": 0.6170589923858643,
      "learning_rate": 3.982407407407407e-06,
      "loss": 0.1583,
      "step": 49700
    },
    {
      "epoch": 2.763888888888889,
      "grad_norm": 2.392925500869751,
      "learning_rate": 3.936111111111111e-06,
      "loss": 0.1859,
      "step": 49750
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 2.3754708766937256,
      "learning_rate": 3.8898148148148145e-06,
      "loss": 0.1548,
      "step": 49800
    },
    {
      "epoch": 2.7694444444444444,
      "grad_norm": 0.8119334578514099,
      "learning_rate": 3.843518518518518e-06,
      "loss": 0.1795,
      "step": 49850
    },
    {
      "epoch": 2.772222222222222,
      "grad_norm": 0.9056912660598755,
      "learning_rate": 3.7972222222222222e-06,
      "loss": 0.1753,
      "step": 49900
    },
    {
      "epoch": 2.775,
      "grad_norm": 0.3548545837402344,
      "learning_rate": 3.750925925925926e-06,
      "loss": 0.16,
      "step": 49950
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 1.1697779893875122,
      "learning_rate": 3.70462962962963e-06,
      "loss": 0.1525,
      "step": 50000
    },
    {
      "epoch": 2.7805555555555554,
      "grad_norm": 0.31185200810432434,
      "learning_rate": 3.6583333333333336e-06,
      "loss": 0.1584,
      "step": 50050
    },
    {
      "epoch": 2.783333333333333,
      "grad_norm": 1.059113621711731,
      "learning_rate": 3.6120370370370372e-06,
      "loss": 0.1535,
      "step": 50100
    },
    {
      "epoch": 2.786111111111111,
      "grad_norm": 2.1790692806243896,
      "learning_rate": 3.565740740740741e-06,
      "loss": 0.1869,
      "step": 50150
    },
    {
      "epoch": 2.7888888888888888,
      "grad_norm": 1.0601965188980103,
      "learning_rate": 3.5194444444444445e-06,
      "loss": 0.1632,
      "step": 50200
    },
    {
      "epoch": 2.7916666666666665,
      "grad_norm": 1.2727550268173218,
      "learning_rate": 3.473148148148148e-06,
      "loss": 0.1456,
      "step": 50250
    },
    {
      "epoch": 2.7944444444444443,
      "grad_norm": 0.6556618213653564,
      "learning_rate": 3.426851851851852e-06,
      "loss": 0.1321,
      "step": 50300
    },
    {
      "epoch": 2.7972222222222225,
      "grad_norm": 1.595976710319519,
      "learning_rate": 3.3805555555555555e-06,
      "loss": 0.1438,
      "step": 50350
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.5807836055755615,
      "learning_rate": 3.334259259259259e-06,
      "loss": 0.1408,
      "step": 50400
    },
    {
      "epoch": 2.802777777777778,
      "grad_norm": 1.421005129814148,
      "learning_rate": 3.2879629629629628e-06,
      "loss": 0.1624,
      "step": 50450
    },
    {
      "epoch": 2.8055555555555554,
      "grad_norm": 1.1152104139328003,
      "learning_rate": 3.2416666666666673e-06,
      "loss": 0.1539,
      "step": 50500
    },
    {
      "epoch": 2.8083333333333336,
      "grad_norm": 2.1468558311462402,
      "learning_rate": 3.195370370370371e-06,
      "loss": 0.1471,
      "step": 50550
    },
    {
      "epoch": 2.811111111111111,
      "grad_norm": 0.6588208675384521,
      "learning_rate": 3.1490740740740745e-06,
      "loss": 0.1677,
      "step": 50600
    },
    {
      "epoch": 2.813888888888889,
      "grad_norm": 1.3417844772338867,
      "learning_rate": 3.102777777777778e-06,
      "loss": 0.1508,
      "step": 50650
    },
    {
      "epoch": 2.8166666666666664,
      "grad_norm": 1.0841532945632935,
      "learning_rate": 3.056481481481482e-06,
      "loss": 0.1456,
      "step": 50700
    },
    {
      "epoch": 2.8194444444444446,
      "grad_norm": 1.2579970359802246,
      "learning_rate": 3.0101851851851855e-06,
      "loss": 0.1574,
      "step": 50750
    },
    {
      "epoch": 2.822222222222222,
      "grad_norm": 0.856766939163208,
      "learning_rate": 2.963888888888889e-06,
      "loss": 0.167,
      "step": 50800
    },
    {
      "epoch": 2.825,
      "grad_norm": 0.6795871257781982,
      "learning_rate": 2.9175925925925928e-06,
      "loss": 0.1516,
      "step": 50850
    },
    {
      "epoch": 2.8277777777777775,
      "grad_norm": 1.327735424041748,
      "learning_rate": 2.8712962962962964e-06,
      "loss": 0.1678,
      "step": 50900
    },
    {
      "epoch": 2.8305555555555557,
      "grad_norm": 1.2063617706298828,
      "learning_rate": 2.825e-06,
      "loss": 0.1529,
      "step": 50950
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 1.199258804321289,
      "learning_rate": 2.7787037037037037e-06,
      "loss": 0.1644,
      "step": 51000
    },
    {
      "epoch": 2.8361111111111112,
      "grad_norm": 1.98928701877594,
      "learning_rate": 2.7324074074074073e-06,
      "loss": 0.1631,
      "step": 51050
    },
    {
      "epoch": 2.838888888888889,
      "grad_norm": 2.744616746902466,
      "learning_rate": 2.686111111111111e-06,
      "loss": 0.174,
      "step": 51100
    },
    {
      "epoch": 2.841666666666667,
      "grad_norm": 0.6989256143569946,
      "learning_rate": 2.639814814814815e-06,
      "loss": 0.1618,
      "step": 51150
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 1.1839922666549683,
      "learning_rate": 2.5935185185185187e-06,
      "loss": 0.1778,
      "step": 51200
    },
    {
      "epoch": 2.8472222222222223,
      "grad_norm": 1.428945541381836,
      "learning_rate": 2.5472222222222224e-06,
      "loss": 0.1715,
      "step": 51250
    },
    {
      "epoch": 2.85,
      "grad_norm": 1.2025020122528076,
      "learning_rate": 2.500925925925926e-06,
      "loss": 0.1382,
      "step": 51300
    },
    {
      "epoch": 2.852777777777778,
      "grad_norm": 0.6998095512390137,
      "learning_rate": 2.4546296296296296e-06,
      "loss": 0.174,
      "step": 51350
    },
    {
      "epoch": 2.8555555555555556,
      "grad_norm": 1.7937346696853638,
      "learning_rate": 2.4083333333333337e-06,
      "loss": 0.1505,
      "step": 51400
    },
    {
      "epoch": 2.8583333333333334,
      "grad_norm": 0.44550880789756775,
      "learning_rate": 2.3620370370370374e-06,
      "loss": 0.164,
      "step": 51450
    },
    {
      "epoch": 2.861111111111111,
      "grad_norm": 1.6343618631362915,
      "learning_rate": 2.315740740740741e-06,
      "loss": 0.1621,
      "step": 51500
    },
    {
      "epoch": 2.863888888888889,
      "grad_norm": 1.916715383529663,
      "learning_rate": 2.2694444444444446e-06,
      "loss": 0.1841,
      "step": 51550
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.6939951777458191,
      "learning_rate": 2.2231481481481483e-06,
      "loss": 0.1776,
      "step": 51600
    },
    {
      "epoch": 2.8694444444444445,
      "grad_norm": 1.557369351387024,
      "learning_rate": 2.176851851851852e-06,
      "loss": 0.1524,
      "step": 51650
    },
    {
      "epoch": 2.8722222222222222,
      "grad_norm": 1.7781504392623901,
      "learning_rate": 2.1305555555555556e-06,
      "loss": 0.1558,
      "step": 51700
    },
    {
      "epoch": 2.875,
      "grad_norm": 1.2249284982681274,
      "learning_rate": 2.0842592592592592e-06,
      "loss": 0.1735,
      "step": 51750
    },
    {
      "epoch": 2.8777777777777778,
      "grad_norm": 3.3294677734375,
      "learning_rate": 2.037962962962963e-06,
      "loss": 0.1472,
      "step": 51800
    },
    {
      "epoch": 2.8805555555555555,
      "grad_norm": 2.1975455284118652,
      "learning_rate": 1.9916666666666665e-06,
      "loss": 0.181,
      "step": 51850
    },
    {
      "epoch": 2.8833333333333333,
      "grad_norm": 1.417685627937317,
      "learning_rate": 1.94537037037037e-06,
      "loss": 0.1408,
      "step": 51900
    },
    {
      "epoch": 2.886111111111111,
      "grad_norm": 1.4595239162445068,
      "learning_rate": 1.8990740740740742e-06,
      "loss": 0.1564,
      "step": 51950
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 1.1494659185409546,
      "learning_rate": 1.8527777777777779e-06,
      "loss": 0.1893,
      "step": 52000
    },
    {
      "epoch": 2.8916666666666666,
      "grad_norm": 0.9988260865211487,
      "learning_rate": 1.8064814814814815e-06,
      "loss": 0.1726,
      "step": 52050
    },
    {
      "epoch": 2.8944444444444444,
      "grad_norm": 0.3750925660133362,
      "learning_rate": 1.7601851851851852e-06,
      "loss": 0.1426,
      "step": 52100
    },
    {
      "epoch": 2.897222222222222,
      "grad_norm": 0.6471111178398132,
      "learning_rate": 1.7138888888888888e-06,
      "loss": 0.1802,
      "step": 52150
    },
    {
      "epoch": 2.9,
      "grad_norm": 1.0886207818984985,
      "learning_rate": 1.6675925925925929e-06,
      "loss": 0.1451,
      "step": 52200
    },
    {
      "epoch": 2.9027777777777777,
      "grad_norm": 1.5901719331741333,
      "learning_rate": 1.6212962962962965e-06,
      "loss": 0.1424,
      "step": 52250
    },
    {
      "epoch": 2.9055555555555554,
      "grad_norm": 2.2933175563812256,
      "learning_rate": 1.5750000000000002e-06,
      "loss": 0.1658,
      "step": 52300
    },
    {
      "epoch": 2.908333333333333,
      "grad_norm": 1.442147135734558,
      "learning_rate": 1.5287037037037038e-06,
      "loss": 0.148,
      "step": 52350
    },
    {
      "epoch": 2.911111111111111,
      "grad_norm": 0.6276780366897583,
      "learning_rate": 1.4824074074074075e-06,
      "loss": 0.158,
      "step": 52400
    },
    {
      "epoch": 2.9138888888888888,
      "grad_norm": 0.8810470700263977,
      "learning_rate": 1.436111111111111e-06,
      "loss": 0.1606,
      "step": 52450
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 1.2048245668411255,
      "learning_rate": 1.3898148148148147e-06,
      "loss": 0.1348,
      "step": 52500
    },
    {
      "epoch": 2.9194444444444443,
      "grad_norm": 1.5911089181900024,
      "learning_rate": 1.3435185185185186e-06,
      "loss": 0.1668,
      "step": 52550
    },
    {
      "epoch": 2.9222222222222225,
      "grad_norm": 0.6121521592140198,
      "learning_rate": 1.2972222222222222e-06,
      "loss": 0.177,
      "step": 52600
    },
    {
      "epoch": 2.925,
      "grad_norm": 1.1296210289001465,
      "learning_rate": 1.250925925925926e-06,
      "loss": 0.1641,
      "step": 52650
    },
    {
      "epoch": 2.927777777777778,
      "grad_norm": 3.592991828918457,
      "learning_rate": 1.2046296296296298e-06,
      "loss": 0.1841,
      "step": 52700
    },
    {
      "epoch": 2.9305555555555554,
      "grad_norm": 10.182745933532715,
      "learning_rate": 1.1583333333333334e-06,
      "loss": 0.1627,
      "step": 52750
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 2.1049273014068604,
      "learning_rate": 1.112037037037037e-06,
      "loss": 0.1577,
      "step": 52800
    },
    {
      "epoch": 2.936111111111111,
      "grad_norm": 0.6510741710662842,
      "learning_rate": 1.0657407407407407e-06,
      "loss": 0.1443,
      "step": 52850
    },
    {
      "epoch": 2.938888888888889,
      "grad_norm": 0.562364399433136,
      "learning_rate": 1.0194444444444445e-06,
      "loss": 0.1718,
      "step": 52900
    },
    {
      "epoch": 2.9416666666666664,
      "grad_norm": 0.5615158081054688,
      "learning_rate": 9.731481481481482e-07,
      "loss": 0.1555,
      "step": 52950
    },
    {
      "epoch": 2.9444444444444446,
      "grad_norm": 0.9707757234573364,
      "learning_rate": 9.268518518518518e-07,
      "loss": 0.1471,
      "step": 53000
    },
    {
      "epoch": 2.947222222222222,
      "grad_norm": 0.8282464146614075,
      "learning_rate": 8.805555555555557e-07,
      "loss": 0.1698,
      "step": 53050
    },
    {
      "epoch": 2.95,
      "grad_norm": 1.2970726490020752,
      "learning_rate": 8.342592592592593e-07,
      "loss": 0.1424,
      "step": 53100
    },
    {
      "epoch": 2.9527777777777775,
      "grad_norm": 0.8554455041885376,
      "learning_rate": 7.879629629629629e-07,
      "loss": 0.1523,
      "step": 53150
    },
    {
      "epoch": 2.9555555555555557,
      "grad_norm": 1.3738881349563599,
      "learning_rate": 7.416666666666667e-07,
      "loss": 0.1691,
      "step": 53200
    },
    {
      "epoch": 2.9583333333333335,
      "grad_norm": 1.6219035387039185,
      "learning_rate": 6.953703703703705e-07,
      "loss": 0.1494,
      "step": 53250
    },
    {
      "epoch": 2.9611111111111112,
      "grad_norm": 1.2307744026184082,
      "learning_rate": 6.490740740740741e-07,
      "loss": 0.1504,
      "step": 53300
    },
    {
      "epoch": 2.963888888888889,
      "grad_norm": 1.1639418601989746,
      "learning_rate": 6.027777777777778e-07,
      "loss": 0.1827,
      "step": 53350
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 1.2076928615570068,
      "learning_rate": 5.564814814814815e-07,
      "loss": 0.1546,
      "step": 53400
    },
    {
      "epoch": 2.9694444444444446,
      "grad_norm": 1.0143427848815918,
      "learning_rate": 5.101851851851853e-07,
      "loss": 0.1715,
      "step": 53450
    },
    {
      "epoch": 2.9722222222222223,
      "grad_norm": 0.8468168377876282,
      "learning_rate": 4.6388888888888886e-07,
      "loss": 0.159,
      "step": 53500
    }
  ],
  "logging_steps": 50,
  "max_steps": 54000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.257924714496e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
